{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **method one**"
      ],
      "metadata": {
        "id": "ciS0_CJN1G4x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Install (run once) ===\n",
        "!pip install -q peft transformers datasets accelerate jiwer evaluate soundfile\n",
        "\n",
        "# === Imports ===\n",
        "import os, json, random, shutil\n",
        "from pathlib import Path\n",
        "import torch\n",
        "from datasets import load_dataset, Audio, Dataset\n",
        "from transformers import (\n",
        "    WhisperProcessor,\n",
        "    WhisperForConditionalGeneration,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    Seq2SeqTrainer\n",
        ")\n",
        "from peft import get_peft_model, LoraConfig, TaskType, PeftModel, PeftConfig\n",
        "import evaluate\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List\n",
        "\n",
        "# === USER CONFIG - edit these paths & hyperparams ===\n",
        "INPUT_JSON = \"all_data.json\"          # your combined JSON manifest (audio_filepath + text)\n",
        "AUDIO_ROOT = \"./audio_files\"          # root where audio files live (if paths in JSON are relative)\n",
        "BASE_MODEL = \"openai/whisper-large-v3-turbo\"\n",
        "SAVED_ADAPTER_DIR = \"turbo_adapter\"   # your existing adapter dir (if you want to resume from it)\n",
        "SAVED_PROCESSOR_DIR = \"turbo_whisper\" # processor dir you saved earlier\n",
        "OUTPUT_ADAPTER_DIR = \"turbo_adapter_updated\"   # where to save updated adapter after training\n",
        "MERGED_MODEL_DIR = \"turbo_whisper_merged\"      # final merged model for direct inference / download\n",
        "ZIP_OUTPUT = MERGED_MODEL_DIR + \".zip\"\n",
        "\n",
        "TRAIN_RATIO = 0.8\n",
        "VALID_RATIO = 0.1\n",
        "TEST_RATIO  = 0.1\n",
        "\n",
        "# Training hyperparams (tune for your GPU)\n",
        "PER_DEVICE_BATCH_SIZE = 4\n",
        "GRAD_ACCUM_STEPS = 8\n",
        "LEARNING_RATE = 3e-5\n",
        "NUM_EPOCHS = 3\n",
        "SAMPLING_RATE = 16000   # set to the rate you used previously\n",
        "\n",
        "# === 1) Split the JSON into train/valid/test ===\n",
        "with open(INPUT_JSON, \"r\", encoding=\"utf-8\") as f:\n",
        "    # support both JSON list and JSONL\n",
        "    try:\n",
        "        data = json.load(f)\n",
        "        if isinstance(data, dict):\n",
        "            # if top-level dict (unexpected), try to find a list field\n",
        "            raise ValueError(\"JSON is a dict — expected a list of examples\")\n",
        "    except Exception:\n",
        "        # fallback: try read lines as JSONL\n",
        "        f.seek(0)\n",
        "        data = [json.loads(line) for line in f if line.strip()]\n",
        "\n",
        "random.shuffle(data)\n",
        "n = len(data)\n",
        "n_train = int(n * TRAIN_RATIO)\n",
        "n_valid = int(n * VALID_RATIO)\n",
        "train_data = data[:n_train]\n",
        "valid_data = data[n_train:n_train + n_valid]\n",
        "test_data  = data[n_train + n_valid:]\n",
        "\n",
        "print(f\"Total examples: {n} → train={len(train_data)}, valid={len(valid_data)}, test={len(test_data)}\")\n",
        "\n",
        "# Save splits to files (used by datasets.load_dataset)\n",
        "os.makedirs(\"splits\", exist_ok=True)\n",
        "for name, arr in ((\"train\", train_data), (\"validation\", valid_data), (\"test\", test_data)):\n",
        "    out_path = os.path.join(\"splits\", f\"{name}.jsonl\")\n",
        "    with open(out_path, \"w\", encoding=\"utf-8\") as fo:\n",
        "        for ex in arr:\n",
        "            # ensure audio path is absolute if needed\n",
        "            if \"audio_filepath\" in ex and not os.path.isabs(ex[\"audio_filepath\"]):\n",
        "                ex[\"audio_filepath\"] = os.path.join(AUDIO_ROOT, ex[\"audio_filepath\"])\n",
        "            fo.write(json.dumps(ex, ensure_ascii=False) + \"\\n\")\n",
        "    print(\"Wrote\", out_path)\n",
        "\n",
        "# === 2) Load processor (tokenizer + feature_extractor) ===\n",
        "processor = WhisperProcessor.from_pretrained(SAVED_PROCESSOR_DIR)\n",
        "\n",
        "# === 3) Load base model and apply LoRA (PEFT) ===\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "base_model = WhisperForConditionalGeneration.from_pretrained(BASE_MODEL)\n",
        "base_model.to(device)\n",
        "\n",
        "# Configure LoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\",\"v_proj\"],   # typical for transformers; adjust if needed for Whisper's module names\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM\n",
        ")\n",
        "\n",
        "# Attach a new LoRA adapter to base model\n",
        "model = get_peft_model(base_model, lora_config)\n",
        "\n",
        "# If you have a previously saved adapter and want to resume from it, load it:\n",
        "if os.path.isdir(SAVED_ADAPTER_DIR):\n",
        "    print(\"Loading existing adapter from\", SAVED_ADAPTER_DIR)\n",
        "    # load PEFT adapter weights onto model\n",
        "    model = PeftModel.from_pretrained(model, SAVED_ADAPTER_DIR, is_trainable=True)\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "# === 4) Load datasets via Hugging Face datasets and preprocess ===\n",
        "data_files = {\n",
        "    \"train\": \"splits/train.jsonl\",\n",
        "    \"validation\": \"splits/validation.jsonl\",\n",
        "    \"test\": \"splits/test.jsonl\"\n",
        "}\n",
        "raw_dsets = load_dataset(\"json\", data_files=data_files)\n",
        "\n",
        "# Cast the \"audio_filepath\" column to Audio to stream/normalize sampling rate\n",
        "# If your JSON column name is \"audio_filepath\", create an \"audio\" column for datasets\n",
        "def load_audio_example(ex):\n",
        "    # datasets Audio expects the column name to be \"audio\" when using cast_column.\n",
        "    # So create a new dict with an 'audio' key pointing to the file path\n",
        "    return {\"audio\": ex[\"audio_filepath\"], **({k:v for k,v in ex.items() if k!=\"audio_filepath\"})}\n",
        "\n",
        "for split in raw_dsets:\n",
        "    raw_dsets[split] = raw_dsets[split].map(load_audio_example)\n",
        "\n",
        "raw_dsets = raw_dsets.cast_column(\"audio\", Audio(sampling_rate=SAMPLING_RATE))\n",
        "\n",
        "# Preprocessing: extract input_features and tokenize targets\n",
        "def preprocess_function(batch):\n",
        "    # batch[\"audio\"][\"array\"] is a numpy array\n",
        "    audio = batch[\"audio\"][\"array\"]\n",
        "    # feature extraction\n",
        "    inputs = processor.feature_extractor(audio, sampling_rate=SAMPLING_RATE, return_tensors=\"pt\")\n",
        "    input_features = inputs.input_features.squeeze(0).numpy()\n",
        "    # tokenize label\n",
        "    with processor.as_target_processor():\n",
        "        labels = processor.tokenizer(batch[\"text\"]).input_ids\n",
        "    return {\"input_features\": input_features, \"labels\": labels}\n",
        "\n",
        "# Map preprocess; do not keep original audio to save memory\n",
        "for split in raw_dsets:\n",
        "    raw_dsets[split] = raw_dsets[split].map(preprocess_function, remove_columns=[\"audio\",\"audio_filepath\"], num_proc=1)\n",
        "\n",
        "# === 5) Data collator ===\n",
        "@dataclass\n",
        "class DataCollatorSpeechSeq2Seq:\n",
        "    processor: WhisperProcessor\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
        "        input_feats = [torch.tensor(f[\"input_features\"], dtype=torch.float32) for f in features]\n",
        "        input_feats = torch.nn.utils.rnn.pad_sequence(input_feats, batch_first=True, padding_value=0.0)\n",
        "        label_ids = [torch.tensor(f[\"labels\"], dtype=torch.long) for f in features]\n",
        "        label_ids = torch.nn.utils.rnn.pad_sequence(label_ids, batch_first=True, padding_value=processor.tokenizer.pad_token_id)\n",
        "        return {\"input_features\": input_feats, \"labels\": label_ids}\n",
        "\n",
        "data_collator = DataCollatorSpeechSeq2Seq(processor=processor)\n",
        "\n",
        "# === 6) Training setup ===\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=OUTPUT_ADAPTER_DIR,\n",
        "    per_device_train_batch_size=PER_DEVICE_BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    logging_steps=50,\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=3,\n",
        "    remove_unused_columns=False,\n",
        "    predict_with_generate=False,\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=raw_dsets[\"train\"],\n",
        "    eval_dataset=raw_dsets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=processor.tokenizer\n",
        ")\n",
        "\n",
        "# === 7) Train ===\n",
        "trainer.train()\n",
        "\n",
        "# === 8) Save updated adapter only (this keeps base model separate) ===\n",
        "os.makedirs(OUTPUT_ADAPTER_DIR, exist_ok=True)\n",
        "model.save_pretrained(OUTPUT_ADAPTER_DIR)\n",
        "processor.save_pretrained(SAVED_PROCESSOR_DIR)\n",
        "print(\"✅ Adapter saved to\", OUTPUT_ADAPTER_DIR)\n",
        "\n",
        "# === 9) Evaluate on the test split (compute WER) ===\n",
        "# For evaluation we will merge adapter into base model for straightforward generation,\n",
        "# or load base model + adapter and generate with .generate().\n",
        "\n",
        "# Option A: merge adapter weights into the base model (this returns a plain model)\n",
        "print(\"Merging adapter into base model for evaluation...\")\n",
        "try:\n",
        "    merged = model.merge_and_unload()  # if supported by PeFT version\n",
        "    # merged is a normal nn.Module without PEFT hooks\n",
        "    merged.save_pretrained(MERGED_MODEL_DIR)\n",
        "    processor.save_pretrained(MERGED_MODEL_DIR)\n",
        "    eval_model = WhisperForConditionalGeneration.from_pretrained(MERGED_MODEL_DIR).to(device)\n",
        "    print(\"Merged model saved to\", MERGED_MODEL_DIR)\n",
        "except Exception as e:\n",
        "    print(\"merge_and_unload not available or failed:\", str(e))\n",
        "    # fallback: use PeftModel instance for generation\n",
        "    eval_model = model\n",
        "    print(\"Will evaluate using PeftModel (adapter attached).\")\n",
        "\n",
        "# Load WER metric\n",
        "wer_metric = evaluate.load(\"wer\")\n",
        "\n",
        "# Function to do generation for a batch (simple loop to avoid memory spikes)\n",
        "def generate_transcript(example):\n",
        "    input_features = torch.tensor(example[\"input_features\"], dtype=torch.float32).unsqueeze(0).to(device)\n",
        "    # prepare model-specific inputs: for Whisper, inputs go via input_features keyword\n",
        "    outputs = eval_model.generate(inputs=input_features, max_new_tokens=256)\n",
        "    pred_text = processor.tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "    return pred_text\n",
        "\n",
        "# Run over test set and compute WER (may be slow; sample or batched if large)\n",
        "refs = []\n",
        "preds = []\n",
        "for i, ex in enumerate(raw_dsets[\"test\"]):\n",
        "    pred = generate_transcript(ex)\n",
        "    preds.append(pred)\n",
        "    refs.append(ex[\"labels\"])  # these are token ids; decode to string\n",
        "    # decode reference\n",
        "    ref_text = processor.tokenizer.decode(ex[\"labels\"], skip_special_tokens=True)\n",
        "    refs[-1] = ref_text\n",
        "    if (i+1) % 50 == 0:\n",
        "        print(f\"Processed {i+1}/{len(raw_dsets['test'])}\")\n",
        "\n",
        "wer_score = wer_metric.compute(predictions=preds, references=refs)\n",
        "print(f\"Test WER: {wer_score:.4f}\")\n",
        "\n",
        "# === 10) Make the merged model easily downloadable (zip the directory) ===\n",
        "if os.path.isdir(MERGED_MODEL_DIR):\n",
        "    shutil.make_archive(MERGED_MODEL_DIR, 'zip', MERGED_MODEL_DIR)\n",
        "    print(\"Zipped merged model to\", ZIP_OUTPUT)\n",
        "    # In many notebook environments (Colab/Jupyter) you can then provide the zip for download:\n",
        "    print(\"Download path:\", os.path.abspath(ZIP_OUTPUT))\n",
        "else:\n",
        "    print(\"Merged model directory not found; adapter-only saved at\", OUTPUT_ADAPTER_DIR)\n",
        "    # zip adapter dir instead\n",
        "    shutil.make_archive(OUTPUT_ADAPTER_DIR, 'zip', OUTPUT_ADAPTER_DIR)\n",
        "    print(\"Zipped adapter to\", OUTPUT_ADAPTER_DIR + \".zip\")\n",
        "    print(\"Download path:\", os.path.abspath(OUTPUT_ADAPTER_DIR + \".zip\"))\n",
        "\n",
        "print(\"All done. ✅\")\n"
      ],
      "metadata": {
        "id": "Ki_qP2gT0GIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **method 2**"
      ],
      "metadata": {
        "id": "3fnsOYr21Olo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Install required packages (run once) ===\n",
        "!pip install -q peft transformers datasets accelerate jiwer evaluate soundfile\n",
        "\n",
        "# === Imports ===\n",
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset, Audio\n",
        "from transformers import (WhisperProcessor,\n",
        "                          WhisperForConditionalGeneration,\n",
        "                          Seq2SeqTrainer,\n",
        "                          Seq2SeqTrainingArguments)\n",
        "from peft import PeftModel, PeftConfig\n",
        "import json\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List\n",
        "\n",
        "# === Paths: change these to your environment ===\n",
        "BASE_MODEL_NAME = \"openai/whisper-large-v3-turbo\"   # or the base model you originally used\n",
        "PEFT_ADAPTER_DIR = \"turbo_adapter\"                  # where you saved adapter\n",
        "PROCESSOR_DIR = \"turbo_whisper\"                     # where you saved processor\n",
        "NEW_JSON = \"new_audio_dataset.json\"                 # your new JSON manifest (audio path + text)\n",
        "AUDIO_ROOT = \"./data_audio\"                         # root dir of the 2000 new audio files\n",
        "OUTPUT_DIR = \"turbo_adapter_updated\"                # where to save updated adapter\n",
        "\n",
        "# === Device ===\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# === Load processor (tokenizer + feature extractor) ===\n",
        "processor = WhisperProcessor.from_pretrained(PROCESSOR_DIR)\n",
        "\n",
        "# === Load base model and attach saved PEFT adapter ===\n",
        "base_model = WhisperForConditionalGeneration.from_pretrained(BASE_MODEL_NAME)\n",
        "# Move to device BEFORE wrapping is sometimes recommended for some workflows:\n",
        "base_model.to(device)\n",
        "\n",
        "# Load the adapter into the base model; keep it trainable\n",
        "# PeftModel.from_pretrained attaches the saved adapter weights to the base model.\n",
        "model = PeftModel.from_pretrained(base_model, PEFT_ADAPTER_DIR, is_trainable=True)\n",
        "model.to(device)\n",
        "\n",
        "# === Load dataset: expect a JSON manifest with fields like {\"audio_filepath\": \"...\", \"text\": \"...\", \"language\":\"hi\"} ===\n",
        "# Example JSON lines shape: [{\"audio_filepath\":\"path/to/file.wav\",\"text\":\"transcript text\",\"id\":\"001\"}, ...]\n",
        "ds = load_dataset(\"json\", data_files=NEW_JSON, split=\"train\")\n",
        "\n",
        "# If your audio paths in the JSON are relative, optionally fix them:\n",
        "def fix_paths(example):\n",
        "    path = example[\"audio_filepath\"]\n",
        "    if not os.path.isabs(path):\n",
        "        example[\"audio_filepath\"] = os.path.join(AUDIO_ROOT, path)\n",
        "    return example\n",
        "\n",
        "ds = ds.map(fix_paths)\n",
        "\n",
        "# Load audio (resamples handled by transformers/datasets)\n",
        "ds = ds.cast_column(\"audio_filepath\", Audio(sampling_rate=16000))  # Use appropriate sample rate\n",
        "\n",
        "# Preprocess: convert audio to input_features and text to labels\n",
        "def preprocess_function(batch):\n",
        "    # 'audio' column may be dict with 'array' + 'sampling_rate'\n",
        "    audio = batch[\"audio_filepath\"][\"array\"]\n",
        "    # extract input features for the model\n",
        "    inputs = processor.feature_extractor(audio, sampling_rate=16000, return_tensors=\"pt\")\n",
        "    input_features = inputs.input_features.squeeze(0).numpy()  # shape: (seq_len, feature_dim) or as required\n",
        "\n",
        "    # prepare labels: use tokenizer from processor\n",
        "    # for Whisper, set language / task if necessary\n",
        "    # tokenizer: processor.tokenizer\n",
        "    with processor.as_target_processor():\n",
        "        labels = processor.tokenizer(batch[\"text\"]).input_ids\n",
        "\n",
        "    return {\"input_features\": input_features, \"labels\": labels}\n",
        "\n",
        "# IMPORTANT: the dataset \"ds\" may be large; map in batched or non-batched depending on memory\n",
        "ds = ds.map(preprocess_function, remove_columns=[\"audio_filepath\", \"audio\"], num_proc=4)\n",
        "\n",
        "# Convert arrays to appropriate torch tensors on the fly by a data collator (see below)\n",
        "\n",
        "# === Data collator ===\n",
        "@dataclass\n",
        "class DataCollatorSpeechSeq2Seq:\n",
        "    processor: WhisperProcessor\n",
        "    pad_to_multiple_of: int = None\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "        # features contain 'input_features' (numpy arrays) and 'labels' (list of ids)\n",
        "        input_features = [torch.tensor(f[\"input_features\"]) for f in features]\n",
        "        input_features = torch.nn.utils.rnn.pad_sequence(input_features, batch_first=True, padding_value=0.0)\n",
        "        # labels: pad with tokenizer.pad_token_id\n",
        "        label_ids = [torch.tensor(f[\"labels\"], dtype=torch.long) for f in features]\n",
        "        label_ids = torch.nn.utils.rnn.pad_sequence(label_ids, batch_first=True, padding_value=processor.tokenizer.pad_token_id)\n",
        "\n",
        "        batch = {\n",
        "            \"input_features\": input_features,\n",
        "            \"labels\": label_ids\n",
        "        }\n",
        "        return batch\n",
        "\n",
        "data_collator = DataCollatorSpeechSeq2Seq(processor=processor)\n",
        "\n",
        "# === TrainingArguments ===\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    per_device_train_batch_size=4,     # reduce if OOM\n",
        "    gradient_accumulation_steps=8,     # simulate larger batch\n",
        "    learning_rate=3e-5,                # try small LR for PEFT\n",
        "    num_train_epochs=3,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    logging_steps=50,\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=3,\n",
        "    predict_with_generate=False,       # set to True if you want validation generation\n",
        ")\n",
        "\n",
        "# === Trainer ===\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=ds,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=processor.tokenizer,   # tokenizer helps with saving & padding\n",
        ")\n",
        "\n",
        "# === Train ===\n",
        "trainer.train()\n",
        "\n",
        "# === Save updated adapter (do NOT overwrite base model) ===\n",
        "# Save PEFT adapter only (this keeps the base model untouched)\n",
        "model.save_pretrained(OUTPUT_DIR)\n",
        "processor.save_pretrained(PROCESSOR_DIR)  # optional: only if processor changed\n",
        "\n",
        "print(\"✅ Updated adapter saved to\", OUTPUT_DIR)\n"
      ],
      "metadata": {
        "id": "K-LYUr_Y1ONX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1umVH-ws0rtA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-H6Lw1vyNNd"
      },
      "source": [
        "<div class=\"markdown-google-sans\">\n",
        "\n",
        "<a name=\"machine-learning-examples\"></a>\n",
        "\n",
        "### Featured examples\n",
        "\n",
        "</div>\n",
        "\n",
        "- [Retraining an Image Classifier](https://tensorflow.org/hub/tutorials/tf2_image_retraining): Build a Keras model on top of a pre-trained image classifier to distinguish flowers.\n",
        "- [Text Classification](https://tensorflow.org/hub/tutorials/tf2_text_classification): Classify IMDB movie reviews as either *positive* or *negative*.\n",
        "- [Style Transfer](https://tensorflow.org/hub/tutorials/tf2_arbitrary_image_stylization): Use deep learning to transfer style between images.\n",
        "- [Multilingual Universal Sentence Encoder Q&A](https://tensorflow.org/hub/tutorials/retrieval_with_tf_hub_universal_encoder_qa): Use a machine learning model to answer questions from the SQuAD dataset.\n",
        "- [Video Interpolation](https://tensorflow.org/hub/tutorials/tweening_conv3d): Predict what happened in a video between the first and the last frame.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
