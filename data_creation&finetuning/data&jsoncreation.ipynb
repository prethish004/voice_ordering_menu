{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SlrdfqaKKVN6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "# --- Load source items ---\n",
        "file_path = \"/mnt/data/Filtered_Unique_Menu_Items (1).csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Use the correct column for item names\n",
        "ITEM_COL = \"Item_Name\"\n",
        "items = df[ITEM_COL].dropna().astype(str).tolist()\n",
        "\n",
        "# --- Quantity word maps (1..10) ---\n",
        "tamil_qty_map = {\n",
        "    1: \"oru\", 2: \"rendu\", 3: \"moonu\", 4: \"naalu\", 5: \"anju\",\n",
        "    6: \"aaru\", 7: \"ezhu\", 8: \"ettu\", 9: \"onpathu\", 10: \"pathu\"\n",
        "}\n",
        "english_qty_map = {\n",
        "    1: \"one\", 2: \"two\", 3: \"three\", 4: \"four\", 5: \"five\",\n",
        "    6: \"six\", 7: \"seven\", 8: \"eight\", 9: \"nine\", 10: \"ten\"\n",
        "}\n",
        "hindi_qty_map = {\n",
        "    1: \"ek\", 2: \"do\", 3: \"teen\", 4: \"char\", 5: \"paanch\",\n",
        "    6: \"chhe\", 7: \"saat\", 8: \"aath\", 9: \"nau\", 10: \"das\"\n",
        "}\n",
        "\n",
        "def sentence_with_random_quantities(items_pool, qty_map, k=None):\n",
        "    \"\"\"\n",
        "    Create a sentence of k items (6-10 default) with randomized order of items\n",
        "    and quantities independently sampled 1..10 *with replacement* (repeats allowed).\n",
        "    \"\"\"\n",
        "    if k is None:\n",
        "        k = random.randint(6, 10)\n",
        "    # random unique items, randomized order\n",
        "    chosen_items = random.sample(items_pool, min(k, len(items_pool)))\n",
        "    # random quantities with replacement\n",
        "    quantities = [random.randint(1, 10) for _ in range(len(chosen_items))]\n",
        "    parts = [f\"{qty_map[q]} {chosen_items[i]}\" for i, q in enumerate(quantities)]\n",
        "    return \" , \".join(parts)\n",
        "\n",
        "def make_lang_df(lang_prefix, lang_name, qty_map, n_sentences=20):\n",
        "    rows = []\n",
        "    for i in range(1, n_sentences + 1):\n",
        "        sent = sentence_with_random_quantities(items, qty_map, k=None)\n",
        "        rows.append((f\"{lang_prefix}_{i:02d}\", sent, lang_name))\n",
        "    return pd.DataFrame(rows, columns=[\"sentence_id\", \"sentence\", \"language\"])\n",
        "\n",
        "# Build dataframes\n",
        "tamil_df = make_lang_df(\"tam\", \"tamil\", tamil_qty_map, 20)\n",
        "english_df = make_lang_df(\"eng\", \"english\", english_qty_map, 20)\n",
        "hindi_df = make_lang_df(\"hin\", \"hindi\", hindi_qty_map, 20)\n",
        "\n",
        "# Save to Excel with three sheets\n",
        "output_excel_path = \"/mnt/data/multilingual_sentences_random_qty.xlsx\"\n",
        "with pd.ExcelWriter(output_excel_path, engine=\"xlsxwriter\") as writer:\n",
        "    tamil_df.to_excel(writer, sheet_name=\"Tamil\", index=False)\n",
        "    english_df.to_excel(writer, sheet_name=\"English\", index=False)\n",
        "    hindi_df.to_excel(writer, sheet_name=\"Hindi\", index=False)\n",
        "\n",
        "# Show previews to user\n",
        "import ace_tools as tools\n",
        "tools.display_dataframe_to_user(\"Tamil (random quantities) - preview\", tamil_df.head(15))\n",
        "tools.display_dataframe_to_user(\"English (random quantities) - preview\", english_df.head(15))\n",
        "tools.display_dataframe_to_user(\"Hindi (random quantities) - preview\", hindi_df.head(15))\n",
        "\n",
        "output_excel_path\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "# --- Config ---\n",
        "INPUT_CSV = \"/mnt/data/Filtered_Unique_Menu_Items (1).csv\"\n",
        "OUTPUT_XLSX = \"/mnt/data/multilingual_sentences_random_qty.xlsx\"\n",
        "NUM_SENTENCES = 20\n",
        "MIN_ITEMS_PER_SENT = 6\n",
        "MAX_ITEMS_PER_SENT = 10\n",
        "ITEM_NAME_COL = \"Item_Name\"  # confirmed earlier\n",
        "\n",
        "# --- Load items ---\n",
        "df_src = pd.read_csv(INPUT_CSV)\n",
        "items = df_src[ITEM_NAME_COL].dropna().astype(str).tolist()\n",
        "\n",
        "# --- Quantity word maps (1..10) ---\n",
        "tamil_qty = {1:\"oru\", 2:\"rendu\", 3:\"moonu\", 4:\"naalu\", 5:\"anju\", 6:\"aaru\", 7:\"ezhu\", 8:\"ettu\", 9:\"onpathu\", 10:\"pathu\"}\n",
        "english_qty = {1:\"one\", 2:\"two\", 3:\"three\", 4:\"four\", 5:\"five\", 6:\"six\", 7:\"seven\", 8:\"eight\", 9:\"nine\", 10:\"ten\"}\n",
        "hindi_qty = {1:\"ek\", 2:\"do\", 3:\"teen\", 4:\"char\", 5:\"paanch\", 6:\"chhe\", 7:\"saat\", 8:\"aath\", 9:\"nau\", 10:\"das\"}\n",
        "\n",
        "def make_sentences(lang_prefix, lang_name, qty_map, count=NUM_SENTENCES):\n",
        "    rows = []\n",
        "    for i in range(1, count + 1):\n",
        "        # choose how many items in this sentence\n",
        "        k = random.randint(MIN_ITEMS_PER_SENT, MAX_ITEMS_PER_SENT)\n",
        "        # unique items per sentence (no item repeats)\n",
        "        selected_items = random.sample(items, k if k <= len(items) else len(items))\n",
        "        # random quantities 1..10, repeats allowed (e.g., 2,7,1,8,2)\n",
        "        qty_nums = [random.randint(1, 10) for _ in range(len(selected_items))]\n",
        "        parts = [f\"{qty_map[q]} {selected_items[j]}\" for j, q in enumerate(qty_nums)]\n",
        "        sentence = \" , \".join(parts)\n",
        "        rows.append((f\"{lang_prefix}_{i:02d}\", sentence, lang_name))\n",
        "    return pd.DataFrame(rows, columns=[\"sentence_id\", \"sentence\", \"language\"])\n",
        "\n",
        "# --- Generate ---\n",
        "tamil_df = make_sentences(\"tam\", \"tamil\", tamil_qty)\n",
        "english_df = make_sentences(\"eng\", \"english\", english_qty)\n",
        "hindi_df = make_sentences(\"hin\", \"hindi\", hindi_qty)\n",
        "\n",
        "# --- Save to Excel with 3 sheets ---\n",
        "with pd.ExcelWriter(OUTPUT_XLSX, engine=\"xlsxwriter\") as writer:\n",
        "    tamil_df.to_excel(writer, sheet_name=\"Tamil\", index=False)\n",
        "    english_df.to_excel(writer, sheet_name=\"English\", index=False)\n",
        "    hindi_df.to_excel(writer, sheet_name=\"Hindi\", index=False)\n",
        "\n",
        "# Show previews to the user\n",
        "import ace_tools as tools\n",
        "tools.display_dataframe_to_user(\"Tamil (preview)\", tamil_df.head(10))\n",
        "tools.display_dataframe_to_user(\"English (preview)\", english_df.head(10))\n",
        "tools.display_dataframe_to_user(\"Hindi (preview)\", hindi_df.head(10))\n",
        "\n",
        "OUTPUT_XLSX\n"
      ],
      "metadata": {
        "id": "Y0s2tXhcKelF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "# --- Config ---\n",
        "INPUT_CSV = \"/mnt/data/Filtered_Unique_Menu_Items (1).csv\"\n",
        "OUTPUT_XLSX = \"/mnt/data/multilingual_sentences_random_qty.xlsx\"\n",
        "NUM_SENTENCES = 20\n",
        "MIN_ITEMS_PER_SENT = 6\n",
        "MAX_ITEMS_PER_SENT = 10\n",
        "ITEM_NAME_COL = \"Item_Name\"  # confirmed earlier\n",
        "\n",
        "# --- Load items ---\n",
        "df_src = pd.read_csv(INPUT_CSV)\n",
        "items = df_src[ITEM_NAME_COL].dropna().astype(str).tolist()\n",
        "\n",
        "# --- Quantity word maps (1..10) ---\n",
        "tamil_qty = {1:\"oru\", 2:\"rendu\", 3:\"moonu\", 4:\"naalu\", 5:\"anju\", 6:\"aaru\", 7:\"ezhu\", 8:\"ettu\", 9:\"onpathu\", 10:\"pathu\"}\n",
        "english_qty = {1:\"one\", 2:\"two\", 3:\"three\", 4:\"four\", 5:\"five\", 6:\"six\", 7:\"seven\", 8:\"eight\", 9:\"nine\", 10:\"ten\"}\n",
        "hindi_qty = {1:\"ek\", 2:\"do\", 3:\"teen\", 4:\"char\", 5:\"paanch\", 6:\"chhe\", 7:\"saat\", 8:\"aath\", 9:\"nau\", 10:\"das\"}\n",
        "\n",
        "def make_sentences(lang_prefix, lang_name, qty_map, count=NUM_SENTENCES):\n",
        "    rows = []\n",
        "    for i in range(1, count + 1):\n",
        "        # choose how many items in this sentence\n",
        "        k = random.randint(MIN_ITEMS_PER_SENT, MAX_ITEMS_PER_SENT)\n",
        "        # unique items per sentence\n",
        "        selected_items = random.sample(items, k if k <= len(items) else len(items))\n",
        "        # random quantities 1..10, repeats allowed (e.g., 2,7,1,8,2)\n",
        "        qty_nums = [random.randint(1, 10) for _ in range(len(selected_items))]\n",
        "        parts = [f\"{qty_map[q]} {selected_items[j]}\" for j, q in enumerate(qty_nums)]\n",
        "        sentence = \" , \".join(parts)\n",
        "        rows.append((f\"{lang_prefix}_{i:02d}\", sentence, lang_name))\n",
        "    return pd.DataFrame(rows, columns=[\"sentence_id\", \"sentence\", \"language\"])\n",
        "\n",
        "# --- Generate ---\n",
        "tamil_df = make_sentences(\"tam\", \"tamil\", tamil_qty)\n",
        "english_df = make_sentences(\"eng\", \"english\", english_qty)\n",
        "hindi_df = make_sentences(\"hin\", \"hindi\", hindi_qty)\n",
        "\n",
        "# --- Save to Excel with 3 sheets ---\n",
        "with pd.ExcelWriter(OUTPUT_XLSX, engine=\"xlsxwriter\") as writer:\n",
        "    tamil_df.to_excel(writer, sheet_name=\"Tamil\", index=False)\n",
        "    english_df.to_excel(writer, sheet_name=\"English\", index=False)\n",
        "    hindi_df.to_excel(writer, sheet_name=\"Hindi\", index=False)\n"
      ],
      "metadata": {
        "id": "aD9dN230KrN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- SETUP ---\n",
        "import os, re, zipfile, json, tempfile\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from google.colab import files\n",
        "\n",
        "# === 1ï¸âƒ£ Upload ZIP ===\n",
        "print(\"ðŸ“ Please upload your ZIP file (with Excel + audio files)...\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "zip_path = list(uploaded.keys())[0]\n",
        "extract_dir = tempfile.mkdtemp(prefix=\"audio_zip_\")\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as z:\n",
        "    z.extractall(extract_dir)\n",
        "\n",
        "print(f\"âœ… Extracted to: {extract_dir}\")\n",
        "\n",
        "# === 2ï¸âƒ£ Find Excel file automatically ===\n",
        "excel_path = None\n",
        "for root, _, fns in os.walk(extract_dir):\n",
        "    for f in fns:\n",
        "        if f.lower().endswith(('.xls', '.xlsx')):\n",
        "            excel_path = os.path.join(root, f)\n",
        "            break\n",
        "    if excel_path: break\n",
        "\n",
        "if not excel_path:\n",
        "    raise FileNotFoundError(\"âŒ No Excel file (.xls/.xlsx) found in the ZIP!\")\n",
        "\n",
        "print(f\"âœ… Found Excel file: {excel_path}\")\n",
        "\n",
        "# === 3ï¸âƒ£ Load Excel sheets (Tamil, English, Hindi) ===\n",
        "def load_sentences(excel_path):\n",
        "    df_list = []\n",
        "    for sheet in [\"Tamil\", \"English\", \"Hindi\"]:\n",
        "        try:\n",
        "            df = pd.read_excel(excel_path, sheet_name=sheet)\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Skipping sheet '{sheet}' ({e})\")\n",
        "            continue\n",
        "        df.columns = [c.strip().lower() for c in df.columns]\n",
        "        if \"sentence_id\" not in df or \"sentence\" not in df:\n",
        "            continue\n",
        "        df[\"sentence_id\"] = df[\"sentence_id\"].astype(str).str.lower()\n",
        "        df[\"sentence\"] = df[\"sentence\"].astype(str)\n",
        "        if \"language\" not in df:\n",
        "            df[\"language\"] = sheet.lower()\n",
        "        df_list.append(df[[\"sentence_id\", \"sentence\", \"language\"]])\n",
        "    all_df = pd.concat(df_list, ignore_index=True)\n",
        "    lookup = {r.sentence_id: r.sentence for r in all_df.itertuples()}\n",
        "    print(f\"âœ… Loaded {len(lookup)} sentences from Excel.\")\n",
        "    return lookup\n",
        "\n",
        "lookup = load_sentences(excel_path)\n",
        "\n",
        "# === 4ï¸âƒ£ Parse audio filenames ===\n",
        "def parse_audio_filename(fname):\n",
        "    \"\"\"\n",
        "    Pattern: <prefix>_<lang>_<NN>.wav  (e.g., 5467868_tam_01.wav)\n",
        "    Returns sentence_id like 'tam_01'\n",
        "    \"\"\"\n",
        "    fname = Path(fname).name\n",
        "    m = re.match(r\"^[A-Za-z0-9\\-]+_([A-Za-z]{2,4})_(\\d{1,3})\\.[A-Za-z0-9]+$\", fname)\n",
        "    if not m:\n",
        "        return None\n",
        "    lang, idx = m.groups()\n",
        "    return f\"{lang.lower()}_{int(idx):02d}\"\n",
        "\n",
        "# === 5ï¸âƒ£ Match audio files to transcripts ===\n",
        "records, unmatched = [], []\n",
        "audio_exts = {\".wav\", \".mp3\", \".flac\", \".m4a\", \".ogg\"}\n",
        "\n",
        "for root, _, files_in_dir in os.walk(extract_dir):\n",
        "    for f in files_in_dir:\n",
        "        if Path(f).suffix.lower() not in audio_exts:\n",
        "            continue\n",
        "        sent_id = parse_audio_filename(f)\n",
        "        abs_path = os.path.join(root, f)\n",
        "        if not sent_id:\n",
        "            unmatched.append({\"file\": abs_path, \"reason\": \"pattern_mismatch\"})\n",
        "            continue\n",
        "        transcript = lookup.get(sent_id)\n",
        "        if transcript:\n",
        "            records.append({\"audiofile_path\": abs_path, \"human_transcript\": transcript})\n",
        "        else:\n",
        "            unmatched.append({\"file\": abs_path, \"reason\": \"no_sentence_match\", \"sentence_id\": sent_id})\n",
        "\n",
        "print(f\"âœ… Matched {len(records)} audio files.\")\n",
        "print(f\"âš ï¸ Unmatched: {len(unmatched)}\")\n",
        "\n",
        "# === 6ï¸âƒ£ Save outputs ===\n",
        "output_json = os.path.join(extract_dir, \"audio_transcripts.json\")\n",
        "with open(output_json, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(records, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "unmatched_json = os.path.join(extract_dir, \"unmatched_files.json\")\n",
        "with open(unmatched_json, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(unmatched, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"âœ… JSON saved to: {output_json}\")\n",
        "\n",
        "# === 7ï¸âƒ£ Download results ===\n",
        "files.download(output_json)\n",
        "files.download(unmatched_json)\n"
      ],
      "metadata": {
        "id": "I-YTm-ntPl0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aiZAwqHXQ3XO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "create_json_from_audio.py\n",
        "\n",
        "Given:\n",
        " - an Excel file with sheets Tamil/English/Hindi containing sentence_id & sentence,\n",
        " - a zip containing that Excel and audio files named like 5467868_tam_01.wav,\n",
        "\n",
        "This script:\n",
        " - extracts the zip into a temporary folder,\n",
        " - reads the Excel sheets into a single dataframe,\n",
        " - scans audio files and attempts to match each file to a sentence_id (language + index),\n",
        " - writes output JSON (list) or JSONL with fields: audiofile_path, human_transcript.\n",
        "\n",
        "Usage:\n",
        " python create_json_from_audio.py --zip input_files.zip --excel \"multilingual_sentences_random_qty.xlsx\" --out out.json\n",
        "\"\"\"\n",
        "\n",
        "import argparse\n",
        "import zipfile\n",
        "import tempfile\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# ----------------------\n",
        "# Helper functions\n",
        "# ----------------------\n",
        "\n",
        "def extract_zip(zip_path, extract_to=None):\n",
        "    \"\"\"Extract zip file and return extraction path.\"\"\"\n",
        "    if extract_to is None:\n",
        "        extract_to = tempfile.mkdtemp(prefix=\"audio_data_\")\n",
        "    with zipfile.ZipFile(zip_path, 'r') as z:\n",
        "        z.extractall(extract_to)\n",
        "    return extract_to\n",
        "\n",
        "def load_sentences_from_excel(excel_path, sheets=('Tamil','English','Hindi')):\n",
        "    \"\"\"\n",
        "    Load sentences from named sheets and return a dataframe with columns:\n",
        "      sentence_id, sentence, language\n",
        "    \"\"\"\n",
        "    df_list = []\n",
        "    for sheet in sheets:\n",
        "        try:\n",
        "            df = pd.read_excel(excel_path, sheet_name=sheet)\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Unable to read sheet '{sheet}' from {excel_path}: {e}\")\n",
        "        # normalize column names to lower for flexibility\n",
        "        cols = {c.lower(): c for c in df.columns}\n",
        "        # required columns\n",
        "        if 'sentence_id' not in cols or 'sentence' not in cols:\n",
        "            raise RuntimeError(f\"Sheet '{sheet}' must contain 'sentence_id' and 'sentence' columns.\")\n",
        "        # ensure language column exists, if not add from sheet name\n",
        "        if 'language' not in cols:\n",
        "            df['language'] = sheet.lower()\n",
        "        else:\n",
        "            # use existing column but normalize values\n",
        "            df['language'] = df[cols['language']].astype(str).str.lower()\n",
        "        # standardize sentence_id -> lower case\n",
        "        df['sentence_id'] = df[cols['sentence_id']].astype(str).str.lower()\n",
        "        df['sentence'] = df[cols['sentence']].astype(str)\n",
        "        df_list.append(df[['sentence_id', 'sentence', 'language']])\n",
        "    combined = pd.concat(df_list, ignore_index=True)\n",
        "    # Make a dictionary for fast lookup: (sentence_id) -> sentence\n",
        "    lookup = {row.sentence_id: row.sentence for row in combined.itertuples()}\n",
        "    return combined, lookup\n",
        "\n",
        "def parse_audio_filename(filename):\n",
        "    \"\"\"\n",
        "    Parse filenames of the form: <prefix>_<lang>_<NN>.<ext>\n",
        "    Returns (prefix, lang_code, index_str) or None on no-match.\n",
        "    Examples matched: 5467868_tam_01.wav, 1234_eng_10.mp3\n",
        "    \"\"\"\n",
        "    fname = Path(filename).name\n",
        "    # Regex: prefix can be digits or alphanum, lang is letters (3), idx is 1-3 digits\n",
        "    m = re.match(r\"^([A-Za-z0-9\\-]+)_([A-Za-z]{2,4})_(\\d{1,3})\\.[A-Za-z0-9]+$\", fname)\n",
        "    if not m:\n",
        "        return None\n",
        "    prefix, lang, idx = m.groups()\n",
        "    # Normalize index to two-digit zero-padded (to match 'tam_01' format)\n",
        "    idx_str = f\"{int(idx):02d}\"\n",
        "    lang_code = lang.lower()\n",
        "    # Build sentence_id: eg \"tam_01\" or \"eng_05\"\n",
        "    sentence_id = f\"{lang_code}_{idx_str}\"\n",
        "    return prefix, lang_code, idx_str, sentence_id\n",
        "\n",
        "def build_json_mapping(audio_dir, lookup):\n",
        "    \"\"\"\n",
        "    Scan audio_dir recursively for audio files, parse filenames and look up transcript.\n",
        "    Returns:\n",
        "      records: list of dicts {audiofile_path, human_transcript}\n",
        "      unmatched: list of file paths that could not be matched to any transcript\n",
        "    \"\"\"\n",
        "    audio_exts = {'.wav', '.mp3', '.flac', '.m4a', '.ogg'}  # extend as needed\n",
        "    records = []\n",
        "    unmatched = []\n",
        "    for root, dirs, files in os.walk(audio_dir):\n",
        "        for f in files:\n",
        "            path = os.path.join(root, f)\n",
        "            if Path(f).suffix.lower() not in audio_exts:\n",
        "                continue\n",
        "            parsed = parse_audio_filename(f)\n",
        "            if not parsed:\n",
        "                unmatched.append({'file': path, 'reason': 'parse_failed'})\n",
        "                continue\n",
        "            prefix, lang_code, idx_str, sentence_id = parsed\n",
        "            # Lookup the sentence text\n",
        "            transcript = lookup.get(sentence_id.lower())\n",
        "            if transcript is None:\n",
        "                unmatched.append({'file': path, 'reason': 'no_sentence_found', 'sentence_id': sentence_id})\n",
        "                continue\n",
        "            # You may want to store a relative path or just filename; here we store full path\n",
        "            records.append({'audiofile_path': os.path.abspath(path), 'human_transcript': transcript})\n",
        "    return records, unmatched\n",
        "\n",
        "# ----------------------\n",
        "# Main CLI\n",
        "# ----------------------\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description=\"Map audio files to transcripts from an Excel and produce JSON.\")\n",
        "    parser.add_argument('--zip', '-z', required=True, help='Input zip file containing audio files and excel (or use --extract-dir instead).')\n",
        "    parser.add_argument('--excel', '-x', required=True, help='Excel file name inside the zip (or an external path).')\n",
        "    parser.add_argument('--out', '-o', default='audio_transcripts.json', help='Output JSON path (array).')\n",
        "    parser.add_argument('--jsonl', action='store_true', help='Write JSONL (one object per line) instead of a JSON array.')\n",
        "    parser.add_argument('--keep-extracted', action='store_true', help='Do not delete extracted temp folder (for debugging).')\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # 1) Extract zip\n",
        "    print(f\"Extracting zip {args.zip} ...\")\n",
        "    extracted_dir = extract_zip(args.zip)\n",
        "    print(f\"Extracted to {extracted_dir}\")\n",
        "\n",
        "    # 2) Locate Excel: prefer path given by --excel inside the extracted folder first, else look absolute\n",
        "    excel_path_in_extracted = os.path.join(extracted_dir, args.excel)\n",
        "    if os.path.exists(excel_path_in_extracted):\n",
        "        excel_path = excel_path_in_extracted\n",
        "    elif os.path.exists(args.excel):\n",
        "        excel_path = args.excel\n",
        "    else:\n",
        "        # try to find an xlsx in the extracted folder if user passed only filename and it's different\n",
        "        found = None\n",
        "        for root, dirs, files in os.walk(extracted_dir):\n",
        "            for f in files:\n",
        "                if f.lower().endswith(('.xls', '.xlsx')) and args.excel.lower() in f.lower():\n",
        "                    found = os.path.join(root, f)\n",
        "                    break\n",
        "            if found:\n",
        "                break\n",
        "        if found:\n",
        "            excel_path = found\n",
        "        else:\n",
        "            raise FileNotFoundError(f\"Excel file '{args.excel}' not found in extracted zip or current dir.\")\n",
        "\n",
        "    print(f\"Using Excel: {excel_path}\")\n",
        "\n",
        "    # 3) Load sentences\n",
        "    combined_df, lookup = load_sentences_from_excel(excel_path)\n",
        "\n",
        "    # 4) Build mapping by scanning extracted dir for audio files\n",
        "    records, unmatched = build_json_mapping(extracted_dir, lookup)\n",
        "\n",
        "    # 5) Write output\n",
        "    print(f\"Found {len(records)} matched audio files, {len(unmatched)} unmatched.\")\n",
        "    if args.jsonl:\n",
        "        with open(args.out, 'w', encoding='utf-8') as fh:\n",
        "            for rec in records:\n",
        "                fh.write(json.dumps(rec, ensure_ascii=False) + '\\n')\n",
        "    else:\n",
        "        with open(args.out, 'w', encoding='utf-8') as fh:\n",
        "            json.dump(records, fh, ensure_ascii=False, indent=2)\n",
        "    print(f\"Wrote {len(records)} records to {args.out}\")\n",
        "\n",
        "    # 6) Save unmatched report (helpful)\n",
        "    unmatched_path = os.path.splitext(args.out)[0] + \"_unmatched.json\"\n",
        "    with open(unmatched_path, 'w', encoding='utf-8') as fh:\n",
        "        json.dump(unmatched, fh, ensure_ascii=False, indent=2)\n",
        "    print(f\"Wrote unmatched report to {unmatched_path}\")\n",
        "\n",
        "    if args.keep_extracted:\n",
        "        print(f\"Extracted folder kept at: {extracted_dir}\")\n",
        "    else:\n",
        "        # optionally remove the extracted_dir (commented out for safety)\n",
        "        # import shutil; shutil.rmtree(extracted_dir)\n",
        "        pass\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "aeFP1RHsQ3av"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}