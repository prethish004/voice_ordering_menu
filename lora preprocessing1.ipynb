{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T11:07:45.427402Z",
     "iopub.status.busy": "2025-10-05T11:07:45.427089Z",
     "iopub.status.idle": "2025-10-05T11:07:45.430288Z",
     "shell.execute_reply": "2025-10-05T11:07:45.429655Z",
     "shell.execute_reply.started": "2025-10-05T11:07:45.427369Z"
    }
   },
   "outputs": [],
   "source": [
    "# pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T11:20:56.873492Z",
     "iopub.status.busy": "2025-10-05T11:20:56.873240Z",
     "iopub.status.idle": "2025-10-05T11:20:56.876247Z",
     "shell.execute_reply": "2025-10-05T11:20:56.875641Z",
     "shell.execute_reply.started": "2025-10-05T11:20:56.873473Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install transformers datasets jiwer soundfile accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T11:07:47.758921Z",
     "iopub.status.busy": "2025-10-05T11:07:47.758660Z",
     "iopub.status.idle": "2025-10-05T11:07:47.761562Z",
     "shell.execute_reply": "2025-10-05T11:07:47.760972Z",
     "shell.execute_reply.started": "2025-10-05T11:07:47.758899Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install onedrivesdk requests tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T11:07:48.356484Z",
     "iopub.status.busy": "2025-10-05T11:07:48.356235Z",
     "iopub.status.idle": "2025-10-05T11:07:48.359064Z",
     "shell.execute_reply": "2025-10-05T11:07:48.358518Z",
     "shell.execute_reply.started": "2025-10-05T11:07:48.356463Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install deep-translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T11:08:43.180082Z",
     "iopub.status.busy": "2025-10-05T11:08:43.179805Z",
     "iopub.status.idle": "2025-10-05T11:08:43.183210Z",
     "shell.execute_reply": "2025-10-05T11:08:43.182606Z",
     "shell.execute_reply.started": "2025-10-05T11:08:43.180061Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 2: Imports\n",
    "# !pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "# conda install -c pytorch torchaudio\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import torchaudio\n",
    "# from deep_translator import GoogleTranslator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T11:08:45.264425Z",
     "iopub.status.busy": "2025-10-05T11:08:45.264161Z",
     "iopub.status.idle": "2025-10-05T11:08:45.423604Z",
     "shell.execute_reply": "2025-10-05T11:08:45.422929Z",
     "shell.execute_reply.started": "2025-10-05T11:08:45.264404Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.12.9\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T11:18:54.035248Z",
     "iopub.status.busy": "2025-10-05T11:18:54.034944Z",
     "iopub.status.idle": "2025-10-05T11:18:55.758950Z",
     "shell.execute_reply": "2025-10-05T11:18:55.758205Z",
     "shell.execute_reply.started": "2025-10-05T11:18:54.035199Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[speech] in /opt/conda/lib/python3.12/site-packages (4.57.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from transformers[speech]) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /opt/conda/lib/python3.12/site-packages (from transformers[speech]) (0.35.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.12/site-packages (from transformers[speech]) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from transformers[speech]) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from transformers[speech]) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.12/site-packages (from transformers[speech]) (2025.9.18)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from transformers[speech]) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/conda/lib/python3.12/site-packages (from transformers[speech]) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.12/site-packages (from transformers[speech]) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.12/site-packages (from transformers[speech]) (4.67.1)\n",
      "Requirement already satisfied: torchaudio in /opt/conda/lib/python3.12/site-packages (from transformers[speech]) (2.8.0)\n",
      "Requirement already satisfied: librosa in /opt/conda/lib/python3.12/site-packages (from transformers[speech]) (0.11.0)\n",
      "Requirement already satisfied: pyctcdecode>=0.4.0 in /opt/conda/lib/python3.12/site-packages (from transformers[speech]) (0.5.0)\n",
      "Requirement already satisfied: phonemizer in /opt/conda/lib/python3.12/site-packages (from transformers[speech]) (3.3.0)\n",
      "Requirement already satisfied: kenlm in /opt/conda/lib/python3.12/site-packages (from transformers[speech]) (0.3.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers[speech]) (2025.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers[speech]) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers[speech]) (1.1.10)\n",
      "Requirement already satisfied: pygtrie<3.0,>=2.1 in /opt/conda/lib/python3.12/site-packages (from pyctcdecode>=0.4.0->transformers[speech]) (2.5.0)\n",
      "Requirement already satisfied: hypothesis<7,>=6.14 in /opt/conda/lib/python3.12/site-packages (from pyctcdecode>=0.4.0->transformers[speech]) (6.140.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.12/site-packages (from hypothesis<7,>=6.14->pyctcdecode>=0.4.0->transformers[speech]) (25.3.0)\n",
      "Requirement already satisfied: sortedcontainers<3.0.0,>=2.1.0 in /opt/conda/lib/python3.12/site-packages (from hypothesis<7,>=6.14->pyctcdecode>=0.4.0->transformers[speech]) (2.4.0)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /opt/conda/lib/python3.12/site-packages (from librosa->transformers[speech]) (3.0.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in /opt/conda/lib/python3.12/site-packages (from librosa->transformers[speech]) (0.62.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.12/site-packages (from librosa->transformers[speech]) (1.16.2)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from librosa->transformers[speech]) (1.7.2)\n",
      "Requirement already satisfied: joblib>=1.0 in /opt/conda/lib/python3.12/site-packages (from librosa->transformers[speech]) (1.5.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.12/site-packages (from librosa->transformers[speech]) (5.2.1)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /opt/conda/lib/python3.12/site-packages (from librosa->transformers[speech]) (0.13.1)\n",
      "Requirement already satisfied: pooch>=1.1 in /opt/conda/lib/python3.12/site-packages (from librosa->transformers[speech]) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /opt/conda/lib/python3.12/site-packages (from librosa->transformers[speech]) (1.0.0)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in /opt/conda/lib/python3.12/site-packages (from librosa->transformers[speech]) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /opt/conda/lib/python3.12/site-packages (from librosa->transformers[speech]) (1.1.1)\n",
      "Requirement already satisfied: llvmlite<0.46,>=0.45.0dev0 in /opt/conda/lib/python3.12/site-packages (from numba>=0.51.0->librosa->transformers[speech]) (0.45.1)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /opt/conda/lib/python3.12/site-packages (from pooch>=1.1->librosa->transformers[speech]) (4.4.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->transformers[speech]) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->transformers[speech]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->transformers[speech]) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->transformers[speech]) (2025.10.5)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn>=1.1.0->librosa->transformers[speech]) (3.6.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.12/site-packages (from soundfile>=0.12.1->librosa->transformers[speech]) (2.0.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.12/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->transformers[speech]) (2.23)\n",
      "Requirement already satisfied: segments in /opt/conda/lib/python3.12/site-packages (from phonemizer->transformers[speech]) (2.3.0)\n",
      "Requirement already satisfied: dlinfo in /opt/conda/lib/python3.12/site-packages (from phonemizer->transformers[speech]) (2.0.0)\n",
      "Requirement already satisfied: csvw>=1.5.6 in /opt/conda/lib/python3.12/site-packages (from segments->phonemizer->transformers[speech]) (3.6.0)\n",
      "Requirement already satisfied: isodate in /opt/conda/lib/python3.12/site-packages (from csvw>=1.5.6->segments->phonemizer->transformers[speech]) (0.7.2)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.12/site-packages (from csvw>=1.5.6->segments->phonemizer->transformers[speech]) (2.9.0.post0)\n",
      "Requirement already satisfied: rfc3986<2 in /opt/conda/lib/python3.12/site-packages (from csvw>=1.5.6->segments->phonemizer->transformers[speech]) (1.5.0)\n",
      "Requirement already satisfied: uritemplate>=3.0.0 in /opt/conda/lib/python3.12/site-packages (from csvw>=1.5.6->segments->phonemizer->transformers[speech]) (4.2.0)\n",
      "Requirement already satisfied: babel in /opt/conda/lib/python3.12/site-packages (from csvw>=1.5.6->segments->phonemizer->transformers[speech]) (2.17.0)\n",
      "Requirement already satisfied: language-tags in /opt/conda/lib/python3.12/site-packages (from csvw>=1.5.6->segments->phonemizer->transformers[speech]) (1.2.0)\n",
      "Requirement already satisfied: rdflib in /opt/conda/lib/python3.12/site-packages (from csvw>=1.5.6->segments->phonemizer->transformers[speech]) (7.2.1)\n",
      "Requirement already satisfied: termcolor in /opt/conda/lib/python3.12/site-packages (from csvw>=1.5.6->segments->phonemizer->transformers[speech]) (3.1.0)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.12/site-packages (from csvw>=1.5.6->segments->phonemizer->transformers[speech]) (4.25.1)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.12/site-packages (from jsonschema->csvw>=1.5.6->segments->phonemizer->transformers[speech]) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.12/site-packages (from jsonschema->csvw>=1.5.6->segments->phonemizer->transformers[speech]) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.12/site-packages (from jsonschema->csvw>=1.5.6->segments->phonemizer->transformers[speech]) (0.27.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil->csvw>=1.5.6->segments->phonemizer->transformers[speech]) (1.17.0)\n",
      "Requirement already satisfied: pyparsing<4,>=2.1.0 in /opt/conda/lib/python3.12/site-packages (from rdflib->csvw>=1.5.6->segments->phonemizer->transformers[speech]) (3.2.5)\n",
      "Requirement already satisfied: torch==2.8.0 in /opt/conda/lib/python3.12/site-packages (from torchaudio->transformers[speech]) (2.8.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch==2.8.0->torchaudio->transformers[speech]) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/conda/lib/python3.12/site-packages (from torch==2.8.0->torchaudio->transformers[speech]) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch==2.8.0->torchaudio->transformers[speech]) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch==2.8.0->torchaudio->transformers[speech]) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /opt/conda/lib/python3.12/site-packages (from torch==2.8.0->torchaudio->transformers[speech]) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /opt/conda/lib/python3.12/site-packages (from torch==2.8.0->torchaudio->transformers[speech]) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /opt/conda/lib/python3.12/site-packages (from torch==2.8.0->torchaudio->transformers[speech]) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /opt/conda/lib/python3.12/site-packages (from torch==2.8.0->torchaudio->transformers[speech]) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /opt/conda/lib/python3.12/site-packages (from torch==2.8.0->torchaudio->transformers[speech]) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /opt/conda/lib/python3.12/site-packages (from torch==2.8.0->torchaudio->transformers[speech]) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /opt/conda/lib/python3.12/site-packages (from torch==2.8.0->torchaudio->transformers[speech]) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /opt/conda/lib/python3.12/site-packages (from torch==2.8.0->torchaudio->transformers[speech]) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /opt/conda/lib/python3.12/site-packages (from torch==2.8.0->torchaudio->transformers[speech]) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /opt/conda/lib/python3.12/site-packages (from torch==2.8.0->torchaudio->transformers[speech]) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /opt/conda/lib/python3.12/site-packages (from torch==2.8.0->torchaudio->transformers[speech]) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /opt/conda/lib/python3.12/site-packages (from torch==2.8.0->torchaudio->transformers[speech]) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /opt/conda/lib/python3.12/site-packages (from torch==2.8.0->torchaudio->transformers[speech]) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /opt/conda/lib/python3.12/site-packages (from torch==2.8.0->torchaudio->transformers[speech]) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /opt/conda/lib/python3.12/site-packages (from torch==2.8.0->torchaudio->transformers[speech]) (3.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy>=1.13.3->torch==2.8.0->torchaudio->transformers[speech]) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch==2.8.0->torchaudio->transformers[speech]) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install \"transformers[speech]\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T11:15:50.706642Z",
     "iopub.status.busy": "2025-10-05T11:15:50.706380Z",
     "iopub.status.idle": "2025-10-05T11:15:50.709412Z",
     "shell.execute_reply": "2025-10-05T11:15:50.708823Z",
     "shell.execute_reply.started": "2025-10-05T11:15:50.706620Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install \"transformers[speech]==4.52.4\" --force-reinstall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-31T13:29:55.442105Z",
     "iopub.status.idle": "2025-08-31T13:29:55.442394Z",
     "shell.execute_reply": "2025-08-31T13:29:55.442253Z",
     "shell.execute_reply.started": "2025-08-31T13:29:55.442241Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys, os, transformers, pkgutil, pathlib\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"transformers version:\", transformers.__version__)\n",
    "\n",
    "# Make sure you don't have a local file/folder named 'transformers' shadowing the package\n",
    "import os, glob\n",
    "print(\"CWD:\", os.getcwd())\n",
    "print(\"Local matches:\", glob.glob(\"./transformers*\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T11:21:10.562424Z",
     "iopub.status.busy": "2025-10-05T11:21:10.562137Z",
     "iopub.status.idle": "2025-10-05T11:21:12.130335Z",
     "shell.execute_reply": "2025-10-05T11:21:12.129644Z",
     "shell.execute_reply.started": "2025-10-05T11:21:10.562403Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: evaluate in /opt/conda/lib/python3.12/site-packages (0.4.6)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.12/site-packages (from evaluate) (4.1.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.12/site-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.12/site-packages (from evaluate) (0.4.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.12/site-packages (from evaluate) (2.3.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.12/site-packages (from evaluate) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.12/site-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.12/site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.12/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.12/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.9.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.12/site-packages (from evaluate) (0.35.3)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from evaluate) (25.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (3.19.1)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /opt/conda/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (21.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.12/site-packages (from fsspec[http]>=2021.05.0->evaluate) (3.12.15)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.2 in /opt/conda/lib/python3.12/site-packages (from aiosignal>=1.4.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (2025.10.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas->evaluate) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-31T13:29:55.443650Z",
     "iopub.status.idle": "2025-08-31T13:29:55.443924Z",
     "shell.execute_reply": "2025-08-31T13:29:55.443798Z",
     "shell.execute_reply.started": "2025-08-31T13:29:55.443786Z"
    }
   },
   "outputs": [],
   "source": [
    "# import boto3\n",
    "\n",
    "# # S3 details\n",
    "# bucket = \"whispersmalltest\"\n",
    "# key = \"OneDrive_1_8-22-2025.zip\"\n",
    "# local_file = \"OneDrive_1_8-22-2025.zip\"\n",
    "\n",
    "# # Download from S3 to local SageMaker Studio storage\n",
    "# s3 = boto3.client(\"s3\")\n",
    "# s3.download_file(bucket, key, local_file)\n",
    "\n",
    "# print(\"File downloaded:\", local_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-31T13:29:55.444387Z",
     "iopub.status.idle": "2025-08-31T13:29:55.444657Z",
     "shell.execute_reply": "2025-08-31T13:29:55.444531Z",
     "shell.execute_reply.started": "2025-08-31T13:29:55.444519Z"
    }
   },
   "outputs": [],
   "source": [
    "# pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T11:08:53.167525Z",
     "iopub.status.busy": "2025-10-05T11:08:53.167240Z",
     "iopub.status.idle": "2025-10-05T11:08:55.376413Z",
     "shell.execute_reply": "2025-10-05T11:08:55.375768Z",
     "shell.execute_reply.started": "2025-10-05T11:08:53.167502Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ffmpeg-python\n",
      "  Using cached ffmpeg_python-0.2.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: imageio[ffmpeg] in /opt/conda/lib/python3.12/site-packages (2.37.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (from imageio[ffmpeg]) (1.26.4)\n",
      "Requirement already satisfied: pillow>=8.3.2 in /opt/conda/lib/python3.12/site-packages (from imageio[ffmpeg]) (11.3.0)\n",
      "Collecting imageio-ffmpeg (from imageio[ffmpeg])\n",
      "  Using cached imageio_ffmpeg-0.6.0-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.12/site-packages (from imageio[ffmpeg]) (5.9.8)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.12/site-packages (from ffmpeg-python) (1.0.0)\n",
      "Using cached ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
      "Using cached imageio_ffmpeg-0.6.0-py3-none-manylinux2014_x86_64.whl (29.5 MB)\n",
      "Installing collected packages: imageio-ffmpeg, ffmpeg-python\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [ffmpeg-python]]\n",
      "\u001b[1A\u001b[2KSuccessfully installed ffmpeg-python-0.2.0 imageio-ffmpeg-0.6.0\n"
     ]
    }
   ],
   "source": [
    "# Update system packages\n",
    "!pip install imageio[ffmpeg] ffmpeg-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T11:10:56.820918Z",
     "iopub.status.busy": "2025-10-05T11:10:56.820634Z",
     "iopub.status.idle": "2025-10-05T11:10:56.838859Z",
     "shell.execute_reply": "2025-10-05T11:10:56.838309Z",
     "shell.execute_reply.started": "2025-10-05T11:10:56.820895Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved train.jsonl (32 records), valid.jsonl (4 records), test.jsonl (4 records))\n"
     ]
    }
   ],
   "source": [
    "import imageio\n",
    "# !pip install imageio-ffmpeg\n",
    "import os\n",
    "import json\n",
    "import zipfile\n",
    "import random\n",
    "\n",
    "# ---- Step 2: Load JSON (outside zip) ----\n",
    "json_file = \"audio_transcripts_updated.json\"  # change this if your JSON has another name\n",
    "\n",
    "if not os.path.exists(json_file):\n",
    "    raise FileNotFoundError(f\"{json_file} not found outside the zip!\")\n",
    "\n",
    "with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# ---- Step 3: Shuffle & Split (80/10/10) ----\n",
    "random.shuffle(data)\n",
    "n = len(data)\n",
    "train_end = int(0.8 * n)\n",
    "valid_end = int(0.9 * n)\n",
    "\n",
    "train_data = data[:train_end]\n",
    "valid_data = data[train_end:valid_end]\n",
    "test_data = data[valid_end:]\n",
    "\n",
    "# ---- Step 4: Save in JSONL format ----\n",
    "def save_jsonl(path, records):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for r in records:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "save_jsonl(\"train.jsonl\", train_data)\n",
    "save_jsonl(\"valid.jsonl\", valid_data)\n",
    "save_jsonl(\"test.jsonl\", test_data)\n",
    "\n",
    "print(f\"✅ Saved train.jsonl ({len(train_data)} records), \"\n",
    "      f\"valid.jsonl ({len(valid_data)} records), \"\n",
    "      f\"test.jsonl ({len(test_data)} records))\")\n",
    "\n",
    "# ---- Step 5: Fix audio file paths ----\n",
    "# def fix_paths(input_path, output_path, base_dir=\"data/\"):\n",
    "#     fixed_lines = []\n",
    "#     with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#         for line in f:\n",
    "#             data = json.loads(line)\n",
    "#             # Fix audio_filepath\n",
    "#             filename = data[\"audio_filepath\"].split(\"/\")[-1]  # keep only filename\n",
    "#             data[\"audio_filepath\"] = base_dir + filename\n",
    "#             fixed_lines.append(json.dumps(data))\n",
    "\n",
    "#     # Write fixed JSONL\n",
    "#     with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "#         for line in fixed_lines:\n",
    "#             f.write(line + \"\\n\")\n",
    "\n",
    "# # Apply to train, valid, and test\n",
    "# fix_paths(\"train.jsonl\", \"train_fixed.jsonl\")\n",
    "# fix_paths(\"valid.jsonl\", \"valid_fixed.jsonl\")\n",
    "# fix_paths(\"test.jsonl\", \"test_fixed.jsonl\")\n",
    "\n",
    "# print(\"✅ Fixed JSONL files created:\")\n",
    "# print(\"Train:\", \"train_fixed.jsonl\")\n",
    "# print(\"Valid:\", \"valid_fixed.jsonl\")\n",
    "# print(\"Test :\", \"test_fixed.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T11:20:21.591512Z",
     "iopub.status.busy": "2025-10-05T11:20:21.591253Z",
     "iopub.status.idle": "2025-10-05T11:20:21.594819Z",
     "shell.execute_reply": "2025-10-05T11:20:21.594251Z",
     "shell.execute_reply.started": "2025-10-05T11:20:21.591493Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.57.0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T11:19:11.953447Z",
     "iopub.status.busy": "2025-10-05T11:19:11.953161Z",
     "iopub.status.idle": "2025-10-05T11:19:15.625310Z",
     "shell.execute_reply": "2025-10-05T11:19:15.624236Z",
     "shell.execute_reply.started": "2025-10-05T11:19:11.953424Z"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'PreTrainedModel' from 'transformers' (/opt/conda/lib/python3.12/site-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset, Dataset\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m WhisperProcessor, WhisperForConditionalGeneration, TrainingArguments, Trainer, Seq2SeqTrainer\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer_callback\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TrainerCallback\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mevaluate\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/transformers/utils/import_utils.py:2317\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2315\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module:\n\u001b[1;32m   2316\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2317\u001b[0m         module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2318\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   2319\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/transformers/utils/import_utils.py:2347\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   2345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   2346\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 2347\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/transformers/utils/import_utils.py:2345\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   2343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   2344\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2345\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2346\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   2347\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/importlib/__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/transformers/trainer.py:42\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING, Any, Callable, Optional, Union\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Integrations must be imported before ML frameworks:\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# ruff: isort: off\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     43\u001b[0m     get_reporting_integration_callbacks,\n\u001b[1;32m     44\u001b[0m )\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# ruff: isort: on\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhuggingface_hub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhf_hub_utils\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/transformers/utils/import_utils.py:2317\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2315\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module:\n\u001b[1;32m   2316\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2317\u001b[0m         module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2318\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   2319\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/transformers/utils/import_utils.py:2347\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   2345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   2346\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 2347\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/transformers/utils/import_utils.py:2345\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   2343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   2344\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2345\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2346\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   2347\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/importlib/__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/transformers/integrations/integration_utils.py:44\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWANDB_MODE\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moffline\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m⚙️  Running in WANDB offline mode\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PreTrainedModel, TrainingArguments\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m version\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     47\u001b[0m     PushToHubMixin,\n\u001b[1;32m     48\u001b[0m     flatten_dict,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     53\u001b[0m     logging,\n\u001b[1;32m     54\u001b[0m )\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'PreTrainedModel' from 'transformers' (/opt/conda/lib/python3.12/site-packages/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "# Cell 2: Import libraries\n",
    "# !pip install -U transformers datasets evaluate accelerate librosa jiwer soundfile\n",
    "\n",
    "import os\n",
    "import torch\n",
    "# import torchaudio\n",
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, TrainingArguments, Trainer, Seq2SeqTrainer\n",
    "from transformers.trainer_callback import TrainerCallback\n",
    "import evaluate\n",
    "from transformers import TrainingArguments\n",
    "import numpy as np\n",
    "from datasets import Dataset, load_dataset\n",
    "import json\n",
    "\n",
    "# Function to load JSONL into Python list\n",
    "# def load_jsonl(path):\n",
    "#     with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "#         return [json.loads(line) for line in f]\n",
    "\n",
    "# train_path = \"/home/sagemaker-user/train.jsonl\"\n",
    "# valid_path = \"/home/sagemaker-user/valid.jsonl\"\n",
    "# Convert to Hugging Face Dataset\n",
    "from datasets import load_dataset\n",
    "# !pip install soundfile\n",
    "\n",
    "train_dataset = load_dataset(\"json\", data_files=\"./train.jsonl\")[\"train\"]\n",
    "valid_dataset = load_dataset(\"json\", data_files=\"./valid.jsonl\")[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-05T11:14:36.996802Z",
     "iopub.status.idle": "2025-10-05T11:14:36.997033Z",
     "shell.execute_reply": "2025-10-05T11:14:36.996932Z",
     "shell.execute_reply.started": "2025-10-05T11:14:36.996922Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install librosa\n",
    "from transformers import WhisperForConditionalGeneration, WhisperProcessor\n",
    "\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-large-v3-turbo\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large-v3-turbo\")\n",
    "from datasets import Audio\n",
    "\n",
    "# Tell HuggingFace to decode audio automatically\n",
    "train_dataset = train_dataset.cast_column(\"audio_filepath\", Audio(sampling_rate=16000))\n",
    "valid_dataset = valid_dataset.cast_column(\"audio_filepath\", Audio(sampling_rate=16000))\n",
    "# !pip install torchcodec\n",
    "def preprocess(example):\n",
    "    # The dataset now gives you {\"array\": np.array, \"sampling_rate\": 16000}\n",
    "    audio = example[\"audio_filepath\"]\n",
    "    input_features = processor.feature_extractor(audio[\"array\"], sampling_rate=16000, return_tensors=\"pt\").input_features[0]\n",
    "    labels = processor.tokenizer(example[\"human_transcript\"]).input_ids\n",
    "\n",
    "    example[\"input_features\"] = input_features\n",
    "    example[\"labels\"] = labels\n",
    "    return example\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess)\n",
    "valid_dataset = valid_dataset.map(preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-05T11:14:36.997602Z",
     "iopub.status.idle": "2025-10-05T11:14:36.997842Z",
     "shell.execute_reply": "2025-10-05T11:14:36.997736Z",
     "shell.execute_reply.started": "2025-10-05T11:14:36.997725Z"
    }
   },
   "outputs": [],
   "source": [
    "def data_collator(features):\n",
    "    # Ensure input_features are also converted to tensors\n",
    "    input_features = torch.stack([torch.tensor(f[\"input_features\"]) for f in features])\n",
    "    label_ids = [f[\"labels\"] for f in features]\n",
    "    labels = processor.tokenizer.pad({\"input_ids\": label_ids}, return_tensors=\"pt\").input_ids\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "    return {\"input_features\": input_features, \"labels\": labels}\n",
    "    wer_metric = evaluate.load(\"wer\")\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    pred_ids = np.argmax(logits, axis=-1)\n",
    "    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = processor.tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    return {\"wer\": wer_metric.compute(predictions=pred_str, references=label_str)}\n",
    "# Cell 8: Trainer callback for logging\n",
    "class LoggingCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs:\n",
    "            print(f\"Epoch {state.epoch:.2f} - Logs: {logs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T04:31:06.298266Z",
     "iopub.status.busy": "2025-09-13T04:31:06.298023Z",
     "iopub.status.idle": "2025-09-13T04:31:06.355973Z",
     "shell.execute_reply": "2025-09-13T04:31:06.355417Z",
     "shell.execute_reply.started": "2025-09-13T04:31:06.298245Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_802/1004993879.py:20: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./fine_tuned_whisper\",\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=5e-5,\n",
    "    logging_dir=\"./logs\",\n",
    "    predict_with_generate=True  # this is important for WER calculation\n",
    ")\n",
    "# training_args = Seq2SeqTrainingArguments(\n",
    "#     output_dir=\"./fine_tuned_whisper\",\n",
    "#     per_device_train_batch_size=2,   # use 2 if your GPU allows\n",
    "#     num_train_epochs=3,              # train for a few epochs\n",
    "#     learning_rate=1e-4,              # slightly higher LR works well for Whisper\n",
    "#     logging_dir=\"./logs\",\n",
    "#     logging_steps=100,\n",
    "#     predict_with_generate=True       # needed for WER\n",
    "# )\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    tokenizer=processor,  # processor includes feature extractor + tokenizer\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T04:31:06.528383Z",
     "iopub.status.busy": "2025-09-13T04:31:06.528183Z",
     "iopub.status.idle": "2025-09-13T04:31:06.786858Z",
     "shell.execute_reply": "2025-09-13T04:31:06.786347Z",
     "shell.execute_reply.started": "2025-09-13T04:31:06.528365Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 7,864,320 || all params: 1,551,169,280 || trainable%: 0.5070\n"
     ]
    }
   ],
   "source": [
    "# !pip install bitsandbytes\n",
    "# !pip install peft \n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "# LoRA Config\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"out_proj\", \"k_proj\"],  # change to [\"out_proj\", \"k_proj\"] if CPU\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "\n",
    "# Apply LoRA to model\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T04:32:55.211540Z",
     "iopub.status.busy": "2025-09-13T04:32:55.211290Z",
     "iopub.status.idle": "2025-09-13T05:41:37.891286Z",
     "shell.execute_reply": "2025-09-13T05:41:37.890796Z",
     "shell.execute_reply.started": "2025-09-13T04:32:55.211521Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1062' max='1062' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1062/1062 1:08:36, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.791800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.256800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/transformers/modeling_utils.py:3852: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1062, training_loss=0.5081760178402543, metrics={'train_runtime': 4120.9522, 'train_samples_per_second': 0.515, 'train_steps_per_second': 0.258, 'total_flos': 4.5336687427584e+18, 'train_loss': 0.5081760178402543, 'epoch': 3.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n",
    "# import torch\n",
    "# torch.cuda.empty_cache()\n",
    "import torch\n",
    "\n",
    "# Clear cache\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# # Optional: reset memory stats\n",
    "# torch.cuda.reset_peak_memory_stats()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T05:42:02.395648Z",
     "iopub.status.busy": "2025-09-13T05:42:02.395394Z",
     "iopub.status.idle": "2025-09-13T05:42:02.804295Z",
     "shell.execute_reply": "2025-09-13T05:42:02.803658Z",
     "shell.execute_reply.started": "2025-09-13T05:42:02.395629Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 12: Save final model\n",
    "model.save_pretrained(\"3epochND_largev2lora_adapter\")\n",
    "processor.save_pretrained(\"3epochND_largev2lora_fine_tuned_whisper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T05:42:03.905637Z",
     "iopub.status.busy": "2025-09-13T05:42:03.905397Z",
     "iopub.status.idle": "2025-09-13T05:42:03.910006Z",
     "shell.execute_reply": "2025-09-13T05:42:03.909454Z",
     "shell.execute_reply.started": "2025-09-13T05:42:03.905617Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install librosa\n",
    "# import json\n",
    "# import torch\n",
    "# import librosa\n",
    "# import evaluate\n",
    "# import pandas as pd\n",
    "# from datasets import Dataset\n",
    "# from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "# # --------------------------------------------------\n",
    "# # Load model and processor\n",
    "# # --------------------------------------------------\n",
    "# model = WhisperForConditionalGeneration.from_pretrained(\n",
    "#     \"1epoch_largev2lora_adapter\"\n",
    "# )\n",
    "# processor = WhisperProcessor.from_pretrained(\n",
    "#     \"1epoch_largev2lora_fine_tuned_whisper\"\n",
    "# )\n",
    "\n",
    "# # Disable forced decoder IDs\n",
    "# model.generation_config.forced_decoder_ids = None\n",
    "# model.generation_config.language = \"en\"   # force English\n",
    "# model.eval()\n",
    "\n",
    "# # --------------------------------------------------\n",
    "# # Load WER metric\n",
    "# # --------------------------------------------------\n",
    "# wer_metric = evaluate.load(\"wer\")\n",
    "\n",
    "# # --------------------------------------------------\n",
    "# # Load validation dataset\n",
    "# # --------------------------------------------------\n",
    "# def load_jsonl(path):\n",
    "#     with open(path, \"r\") as f:\n",
    "#         return [json.loads(line) for line in f]\n",
    "\n",
    "# data_list = load_jsonl(\"test.jsonl\")\n",
    "# data_dict = {k: [d[k] for d in data_list] for k in data_list[0]}\n",
    "# valid_dataset = Dataset.from_dict(data_dict)\n",
    "\n",
    "# # --------------------------------------------------\n",
    "# # Transcribe & Compare\n",
    "# # --------------------------------------------------\n",
    "# def transcribe_and_compare(dataset):\n",
    "#     predictions, references, files = [], [], []\n",
    "\n",
    "#     for example in dataset:\n",
    "#         try:\n",
    "#             # 🔹 Load & resample audio to 16kHz\n",
    "#             waveform, sr = librosa.load(example[\"audio_filepath\"], sr=16000)\n",
    "\n",
    "#             inputs = processor(\n",
    "#                 waveform, sampling_rate=16000, return_tensors=\"pt\"\n",
    "#             )\n",
    "#             input_features = inputs.input_features\n",
    "\n",
    "#             with torch.no_grad():\n",
    "#                 predicted_ids = model.generate(input_features=input_features)\n",
    "\n",
    "#             pred_text = processor.batch_decode(\n",
    "#                 predicted_ids, skip_special_tokens=True\n",
    "#             )[0].strip()\n",
    "\n",
    "#             predictions.append(pred_text)\n",
    "#             references.append(example[\"human_transcript\"].strip())\n",
    "#             files.append(example[\"audio_filepath\"])\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"❌ Skipping {example['audio_filepath']}: {e}\")\n",
    "\n",
    "#     if not predictions:\n",
    "#         print(\"⚠️ No valid transcriptions found.\")\n",
    "#         return\n",
    "\n",
    "#     # Compute WER\n",
    "#     wer = wer_metric.compute(predictions=predictions, references=references)\n",
    "#     print(f\"\\n✅ Final Word Error Rate (WER): {wer:.3f}\")\n",
    "\n",
    "#     # Side-by-side comparison\n",
    "#     df = pd.DataFrame({\n",
    "#         \"Audio File\": files,\n",
    "#         \"Reference Transcript\": references,\n",
    "#         \"Model Transcript\": predictions\n",
    "#     })\n",
    "\n",
    "#     from IPython.display import display\n",
    "#     display(df)\n",
    "\n",
    "#     return df\n",
    "\n",
    "# # --------------------------------------------------\n",
    "# # Run transcription\n",
    "# # --------------------------------------------------\n",
    "# df = transcribe_and_compare(valid_dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T05:42:04.323082Z",
     "iopub.status.busy": "2025-09-13T05:42:04.322845Z",
     "iopub.status.idle": "2025-09-13T05:42:07.904429Z",
     "shell.execute_reply": "2025-09-13T05:42:07.903886Z",
     "shell.execute_reply.started": "2025-09-13T05:42:04.323064Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForSeq2SeqLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): WhisperForConditionalGeneration(\n",
       "      (model): WhisperModel(\n",
       "        (encoder): WhisperEncoder(\n",
       "          (conv1): Conv1d(80, 1280, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (conv2): Conv1d(1280, 1280, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "          (embed_positions): Embedding(1500, 1280)\n",
       "          (layers): ModuleList(\n",
       "            (0-31): 32 x WhisperEncoderLayer(\n",
       "              (self_attn): WhisperAttention(\n",
       "                (k_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1280, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=1280, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (out_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1280, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=1280, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "              )\n",
       "              (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (activation_fn): GELUActivation()\n",
       "              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (decoder): WhisperDecoder(\n",
       "          (embed_tokens): Embedding(51865, 1280, padding_idx=50257)\n",
       "          (embed_positions): WhisperPositionalEmbedding(448, 1280)\n",
       "          (layers): ModuleList(\n",
       "            (0-31): 32 x WhisperDecoderLayer(\n",
       "              (self_attn): WhisperAttention(\n",
       "                (k_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1280, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=1280, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (out_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1280, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=1280, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "              )\n",
       "              (activation_fn): GELUActivation()\n",
       "              (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (encoder_attn): WhisperAttention(\n",
       "                (k_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1280, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=1280, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (out_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1280, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=1280, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "              )\n",
       "              (encoder_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (proj_out): Linear(in_features=1280, out_features=51865, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # import json\n",
    "# # import torch\n",
    "# # import librosa\n",
    "# # import evaluate\n",
    "# # import pandas as pd\n",
    "# # from datasets import Dataset\n",
    "# # from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "# # # --------------------------------------------------\n",
    "# # # Load model and processor\n",
    "# # # --------------------------------------------------\n",
    "# # model = WhisperForConditionalGeneration.from_pretrained(\n",
    "# #     \"/home/sagemaker-user/1epoch_largelora_adapter\"\n",
    "# # )\n",
    "# # processor = WhisperProcessor.from_pretrained(\n",
    "# #     \"/home/sagemaker-user/1epoch_largelora_fine_tuned_whisper\"\n",
    "# # )\n",
    "\n",
    "# # # Disable forced decoder IDs\n",
    "# # model.generation_config.forced_decoder_ids = None\n",
    "# # model.generation_config.language = \"en\"   # force English\n",
    "# # model.eval()\n",
    "\n",
    "# # # --------------------------------------------------\n",
    "# # # Load WER metric\n",
    "# # # --------------------------------------------------\n",
    "# # wer_metric = evaluate.load(\"wer\")\n",
    "\n",
    "# # # --------------------------------------------------\n",
    "# # # Load validation dataset\n",
    "# # # --------------------------------------------------\n",
    "# # def load_jsonl(path):\n",
    "# #     with open(path, \"r\") as f:\n",
    "# #         return [json.loads(line) for line in f]\n",
    "\n",
    "# # data_list = load_jsonl(\"valid.jsonl\")\n",
    "# # data_dict = {k: [d[k] for d in data_list] for k in data_list[0]}\n",
    "# # valid_dataset = Dataset.from_dict(data_dict)\n",
    "\n",
    "# # # --------------------------------------------------\n",
    "# # # Transcribe & Compare\n",
    "# # # --------------------------------------------------\n",
    "# # def transcribe_and_compare(dataset):\n",
    "# #     predictions, references, files = [], [], []\n",
    "\n",
    "# #     for example in dataset:\n",
    "# #         try:\n",
    "# #             # 🔹 Load & resample audio to 16kHz\n",
    "# #             waveform, sr = librosa.load(example[\"audio_filepath\"], sr=16000)\n",
    "\n",
    "# #             inputs = processor(\n",
    "# #                 waveform, sampling_rate=16000, return_tensors=\"pt\"\n",
    "# #             )\n",
    "# #             input_features = inputs.input_features\n",
    "\n",
    "# #             with torch.no_grad():\n",
    "# #                 predicted_ids = model.generate(input_features=input_features)\n",
    "\n",
    "# #             pred_text = processor.batch_decode(\n",
    "# #                 predicted_ids, skip_special_tokens=True\n",
    "# #             )[0].strip()\n",
    "\n",
    "# #             predictions.append(pred_text)\n",
    "# #             references.append(example[\"human_transcript\"].strip())\n",
    "# #             files.append(example[\"audio_filepath\"])\n",
    "\n",
    "# #         except Exception as e:\n",
    "# #             print(f\"❌ Skipping {example['audio_filepath']}: {e}\")\n",
    "\n",
    "# #     if not predictions:\n",
    "# #         print(\"⚠️ No valid transcriptions found.\")\n",
    "# #         return\n",
    "\n",
    "# #     # Compute WER\n",
    "# #     wer = wer_metric.compute(predictions=predictions, references=references)\n",
    "# #     print(f\"\\n✅ Final Word Error Rate (WER): {wer:.3f}\")\n",
    "\n",
    "# #     # Side-by-side comparison\n",
    "# #     df = pd.DataFrame({\n",
    "# #         \"Audio File\": files,\n",
    "# #         \"Reference Transcript\": references,\n",
    "# #         \"Model Transcript\": predictions\n",
    "# #     })\n",
    "\n",
    "# #     from IPython.display import display\n",
    "# #     display(df)\n",
    "\n",
    "# #     return df\n",
    "\n",
    "\n",
    "# # # --------------------------------------------------\n",
    "# # # Run transcription\n",
    "# # # --------------------------------------------------\n",
    "# # df = transcribe_and_compare(valid_dataset)\n",
    "# import json\n",
    "# import torch\n",
    "# import librosa\n",
    "# import evaluate\n",
    "# import pandas as pd\n",
    "# from datasets import Dataset\n",
    "# from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "# # --------------------------------------------------\n",
    "# # Load model and processor (choose large, large-v2, or large-v3)\n",
    "# # --------------------------------------------------\n",
    "# MODEL_PATH = \"/home/sagemaker-user/1epoch_largelora_adapter\"\n",
    "# PROCESSOR_PATH = \"/home/sagemaker-user/1epoch_largelora_fine_tuned_whisper\"\n",
    "\n",
    "# model = WhisperForConditionalGeneration.from_pretrained(MODEL_PATH)\n",
    "# processor = WhisperProcessor.from_pretrained(PROCESSOR_PATH)\n",
    "\n",
    "# # Disable forced language/decoder IDs (so predictions aren't biased)\n",
    "# model.generation_config.forced_decoder_ids = None\n",
    "# model.generation_config.suppress_tokens = []\n",
    "# model.generation_config.language = \"en\"\n",
    "# model.generation_config.task = \"transcribe\"\n",
    "# model.eval()\n",
    "\n",
    "# # --------------------------------------------------\n",
    "# # Metric\n",
    "# # --------------------------------------------------\n",
    "# wer_metric = evaluate.load(\"wer\")\n",
    "\n",
    "# # --------------------------------------------------\n",
    "# # Load validation dataset\n",
    "# # --------------------------------------------------\n",
    "# def load_jsonl(path: str):\n",
    "#     \"\"\"Load jsonl file with {'audio_filepath': ..., 'human_transcript': ...}\"\"\"\n",
    "#     with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "#         return [json.loads(line) for line in f]\n",
    "\n",
    "# data_list = load_jsonl(\"valid.jsonl\")\n",
    "# data_dict = {k: [d[k] for d in data_list] for k in data_list[0]}\n",
    "# valid_dataset = Dataset.from_dict(data_dict)\n",
    "\n",
    "# # --------------------------------------------------\n",
    "# # Transcribe & Evaluate\n",
    "# # --------------------------------------------------\n",
    "# def transcribe_and_compare(dataset: Dataset):\n",
    "#     predictions, references, files = [], [], []\n",
    "\n",
    "#     for example in dataset:\n",
    "#         try:\n",
    "#             # Load & resample audio to 16kHz\n",
    "#             waveform, _ = librosa.load(example[\"audio_filepath\"], sr=16000)\n",
    "\n",
    "#             # Prepare input features\n",
    "#             inputs = processor(\n",
    "#                 waveform, sampling_rate=16000, return_tensors=\"pt\"\n",
    "#             )\n",
    "#             input_features = inputs.input_features\n",
    "\n",
    "#             with torch.no_grad():\n",
    "#                 predicted_ids = model.generate(\n",
    "#                     input_features=input_features,\n",
    "#                     max_length=448,   # prevent runaway generations\n",
    "#                 )\n",
    "\n",
    "#             pred_text = processor.batch_decode(\n",
    "#                 predicted_ids, skip_special_tokens=True\n",
    "#             )[0].strip()\n",
    "\n",
    "#             predictions.append(pred_text)\n",
    "#             references.append(example[\"human_transcript\"].strip())\n",
    "#             files.append(example[\"audio_filepath\"])\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"❌ Skipping {example.get('audio_filepath', 'UNKNOWN')}: {e}\")\n",
    "\n",
    "#     if not predictions:\n",
    "#         print(\"⚠️ No valid transcriptions found.\")\n",
    "#         return None\n",
    "\n",
    "#     # Compute WER\n",
    "#     wer = wer_metric.compute(predictions=predictions, references=references)\n",
    "#     print(f\"\\n✅ Final Word Error Rate (WER): {wer:.3f}\")\n",
    "\n",
    "#     # Side-by-side comparison\n",
    "#     df = pd.DataFrame({\n",
    "#         \"Audio File\": files,\n",
    "#         \"Reference Transcript\": references,\n",
    "#         \"Model Transcript\": predictions\n",
    "#     })\n",
    "\n",
    "#     try:\n",
    "#         from IPython.display import display\n",
    "#         display(df)\n",
    "#     except ImportError:\n",
    "#         print(df.head())\n",
    "\n",
    "#     return df\n",
    "\n",
    "\n",
    "# # --------------------------------------------------\n",
    "# # Run transcription\n",
    "# # --------------------------------------------------\n",
    "# if __name__ == \"__main__\":\n",
    "#     df = transcribe_and_compare(valid_dataset)\n",
    "# !pip install peft\n",
    "import json\n",
    "import torch\n",
    "import librosa\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "from peft import PeftModel\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Base model + LoRA adapter\n",
    "# --------------------------------------------------\n",
    "BASE_MODEL = \"openai/whisper-large-v2\"  # or \"openai/whisper-large-v3\"\n",
    "ADAPTER_PATH = \"3epochND_largev2lora_adapter\"\n",
    "PROCESSOR_PATH = \"3epochND_largev2lora_fine_tuned_whisper\"\n",
    "\n",
    "# Load base model\n",
    "model = WhisperForConditionalGeneration.from_pretrained(BASE_MODEL)\n",
    "\n",
    "# Attach LoRA adapter\n",
    "model = PeftModel.from_pretrained(model, ADAPTER_PATH)\n",
    "\n",
    "# Load processor (tokenizer + feature extractor)\n",
    "processor = WhisperProcessor.from_pretrained(PROCESSOR_PATH)\n",
    "\n",
    "# Generation config\n",
    "model.generation_config.forced_decoder_ids = None\n",
    "model.generation_config.suppress_tokens = []\n",
    "model.generation_config.language = \"en\"\n",
    "model.generation_config.task = \"transcribe\"\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T05:45:15.217804Z",
     "iopub.status.busy": "2025-09-13T05:45:15.217626Z",
     "iopub.status.idle": "2025-09-13T06:53:44.977577Z",
     "shell.execute_reply": "2025-09-13T06:53:44.976992Z",
     "shell.execute_reply.started": "2025-09-13T05:45:15.217787Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jiwer in /opt/conda/lib/python3.12/site-packages (4.0.0)\n",
      "Requirement already satisfied: click>=8.1.8 in /opt/conda/lib/python3.12/site-packages (from jiwer) (8.2.2)\n",
      "Requirement already satisfied: rapidfuzz>=3.9.7 in /opt/conda/lib/python3.12/site-packages (from jiwer) (3.14.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 05:45:25.701990: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-09-13 05:45:25.718726: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1757742325.736144    2269 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1757742325.741963    2269 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-09-13 05:45:25.759495: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Loading base Whisper model...\n",
      "🔹 Applying LoRA adapter...\n",
      "🔹 Loading processor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Final Word Error Rate (WER): 6.164\n",
      "📂 Results saved to transcriptions.txt\n"
     ]
    }
   ],
   "source": [
    "!pip install jiwer\n",
    "import json\n",
    "import torch\n",
    "import librosa\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "from peft import PeftModel\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Paths\n",
    "# --------------------------------------------------\n",
    "BASE_MODEL = \"openai/whisper-large-v2\"   # or \"openai/whisper-large-v3\"\n",
    "ADAPTER_PATH = \"3epochND_largev2lora_adapter\"\n",
    "PROCESSOR_PATH = \"3epochND_largev2lora_fine_tuned_whisper\"\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Load model + processor\n",
    "# --------------------------------------------------\n",
    "print(\"🔹 Loading base Whisper model...\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(BASE_MODEL)\n",
    "\n",
    "print(\"🔹 Applying LoRA adapter...\")\n",
    "model = PeftModel.from_pretrained(model, ADAPTER_PATH)\n",
    "\n",
    "print(\"🔹 Loading processor...\")\n",
    "processor = WhisperProcessor.from_pretrained(PROCESSOR_PATH)\n",
    "\n",
    "# Disable forced decoder IDs (so predictions aren’t biased)\n",
    "model.generation_config.forced_decoder_ids = None\n",
    "model.generation_config.suppress_tokens = []\n",
    "model.generation_config.language = \"en\"\n",
    "model.generation_config.task = \"transcribe\"\n",
    "model.eval()\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Load WER metric\n",
    "# --------------------------------------------------\n",
    "wer_metric = evaluate.load(\"wer\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Load validation dataset\n",
    "# --------------------------------------------------\n",
    "def load_jsonl(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "data_list = load_jsonl(\"csv files/test.jsonl\")\n",
    "data_dict = {k: [d[k] for d in data_list] for k in data_list[0]}\n",
    "valid_dataset = Dataset.from_dict(data_dict)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Transcribe & Compare\n",
    "# --------------------------------------------------\n",
    "# def transcribe_and_compare(dataset):\n",
    "#     predictions, references, files = [], [], []\n",
    "\n",
    "#     for example in dataset:\n",
    "#         try:\n",
    "#             # Load & resample audio to 16kHz\n",
    "#             waveform, sr = librosa.load(example[\"audio_filepath\"], sr=16000)\n",
    "\n",
    "#             inputs = processor(waveform, sampling_rate=16000, return_tensors=\"pt\")\n",
    "#             input_features = inputs.input_features\n",
    "\n",
    "#             with torch.no_grad():\n",
    "#                 predicted_ids = model.generate(input_features=input_features)\n",
    "\n",
    "#             pred_text = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0].strip()\n",
    "\n",
    "#             predictions.append(pred_text)\n",
    "#             references.append(example[\"human_transcript\"].strip())\n",
    "#             files.append(example[\"audio_filepath\"])\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"❌ Skipping {example['audio_filepath']}: {e}\")\n",
    "\n",
    "#     if not predictions:\n",
    "#         print(\"⚠️ No valid transcriptions found.\")\n",
    "#         return\n",
    "\n",
    "#     # Compute WER\n",
    "#     wer = wer_metric.compute(predictions=predictions, references=references)\n",
    "#     print(f\"\\n✅ Final Word Error Rate (WER): {wer:.3f}\")\n",
    "\n",
    "#     # Side-by-side comparison\n",
    "#     df = pd.DataFrame({\n",
    "#         \"Audio File\": files,\n",
    "#         \"Reference Transcript\": references,\n",
    "#         \"Model Transcript\": predictions\n",
    "#     })\n",
    "\n",
    "#     from IPython.display import display\n",
    "#     display(df)\n",
    "\n",
    "#     return df\n",
    "\n",
    "# # --------------------------------------------------\n",
    "# # Run transcription\n",
    "# # --------------------------------------------------\n",
    "# df = transcribe_and_compare(valid_dataset)\n",
    "# --------------------------------------------------\n",
    "# Transcribe & Compare\n",
    "# --------------------------------------------------\n",
    "def transcribe_and_compare(dataset, output_txt=\"transcriptions.txt\"):\n",
    "    predictions, references, files = [], [], []\n",
    "\n",
    "    for example in dataset:\n",
    "        try:\n",
    "            # Load & resample audio to 16kHz\n",
    "            waveform, sr = librosa.load(example[\"audio_filepath\"], sr=16000)\n",
    "\n",
    "            inputs = processor(waveform, sampling_rate=16000, return_tensors=\"pt\")\n",
    "            input_features = inputs.input_features\n",
    "\n",
    "            with torch.no_grad():\n",
    "                predicted_ids = model.generate(input_features=input_features)\n",
    "\n",
    "            pred_text = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0].strip()\n",
    "\n",
    "            predictions.append(pred_text)\n",
    "            references.append(example[\"human_transcript\"].strip())\n",
    "            files.append(example[\"audio_filepath\"])\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Skipping {example['audio_filepath']}: {e}\")\n",
    "\n",
    "    if not predictions:\n",
    "        print(\"⚠️ No valid transcriptions found.\")\n",
    "        return\n",
    "\n",
    "    # Compute WER\n",
    "    wer = wer_metric.compute(predictions=predictions, references=references)\n",
    "    print(f\"\\n✅ Final Word Error Rate (WER): {wer:.3f}\")\n",
    "\n",
    "    # Save results to TXT file\n",
    "    with open(output_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"Final WER: {wer:.3f}\\n\\n\")\n",
    "        f.write(\"Audio File\\tReference Transcript\\tModel Transcript\\n\")\n",
    "        f.write(\"=\" * 120 + \"\\n\")\n",
    "        for file, ref, pred in zip(files, references, predictions):\n",
    "            f.write(f\"{file}\\nREF: {ref}\\nPRED: {pred}\\n\\n\")\n",
    "\n",
    "    print(f\"📂 Results saved to {output_txt}\")\n",
    "    return output_txt\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Run transcription\n",
    "# --------------------------------------------------\n",
    "txt_file = transcribe_and_compare(valid_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T06:53:44.980054Z",
     "iopub.status.busy": "2025-09-13T06:53:44.979836Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Loading baseline Whisper model...\n",
      "\n",
      "=== Fine-tuned LoRA Whisper ===\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------\n",
    "# Baseline: plain Whisper-large-v2\n",
    "# --------------------------------------------------\n",
    "print(\"🔹 Loading baseline Whisper model...\")\n",
    "baseline_model = WhisperForConditionalGeneration.from_pretrained(BASE_MODEL)\n",
    "baseline_processor = WhisperProcessor.from_pretrained(BASE_MODEL)\n",
    "\n",
    "baseline_model.generation_config.forced_decoder_ids = None\n",
    "baseline_model.generation_config.suppress_tokens = []\n",
    "baseline_model.generation_config.language = \"en\"\n",
    "baseline_model.generation_config.task = \"transcribe\"\n",
    "baseline_model.eval()\n",
    "\n",
    "def transcribe_with_model(dataset, model, processor, name=\"Model\"):\n",
    "    predictions, references, files = [], [], []\n",
    "\n",
    "    for example in dataset:\n",
    "        try:\n",
    "            waveform, sr = librosa.load(example[\"audio_filepath\"], sr=16000)\n",
    "\n",
    "            inputs = processor(waveform, sampling_rate=16000, return_tensors=\"pt\")\n",
    "            input_features = inputs.input_features\n",
    "\n",
    "            with torch.no_grad():\n",
    "                predicted_ids = model.generate(input_features=input_features)\n",
    "\n",
    "            pred_text = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0].strip()\n",
    "\n",
    "            predictions.append(pred_text)\n",
    "            references.append(example[\"human_transcript\"].strip())\n",
    "            files.append(example[\"audio_filepath\"])\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Skipping {example['audio_filepath']}: {e}\")\n",
    "\n",
    "    if not predictions:\n",
    "        print(f\"⚠️ No valid transcriptions found for {name}.\")\n",
    "        return None\n",
    "\n",
    "    wer = wer_metric.compute(predictions=predictions, references=references)\n",
    "    print(f\"\\n✅ {name} Word Error Rate (WER): {wer:.3f}\")\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"Audio File\": files,\n",
    "        f\"{name} Transcript\": predictions,\n",
    "        \"Reference Transcript\": references\n",
    "    })\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Evaluate both models\n",
    "# --------------------------------------------------\n",
    "print(\"\\n=== Fine-tuned LoRA Whisper ===\")\n",
    "model.generation_config.forced_decoder_ids = None\n",
    "model.generation_config.suppress_tokens = []\n",
    "model.generation_config.language = \"en\"\n",
    "model.generation_config.task = \"transcribe\"\n",
    "\n",
    "df_finetuned = transcribe_with_model(valid_dataset, model, processor, \"Fine-tuned Whisper\")\n",
    "\n",
    "print(\"\\n=== Baseline Whisper-large-v2 ===\")\n",
    "df_baseline = transcribe_with_model(valid_dataset, baseline_model, baseline_processor, \"Baseline Whisper\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Merge results for side-by-side comparison\n",
    "# --------------------------------------------------\n",
    "if df_finetuned is not None and df_baseline is not None:\n",
    "    df_compare = df_finetuned.merge(\n",
    "        df_baseline[[\"Audio File\", \"Baseline Whisper Transcript\"]],\n",
    "        on=\"Audio File\"\n",
    "    )\n",
    "    from IPython.display import display\n",
    "    display(df_compare)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Load a single test audio\n",
    "# --------------------------------------------------\n",
    "AUDIO_PATH = \"data/SENT_0905.wav\"\n",
    "REFERENCE = \"rendu Brownie, oru Akkaravadisal, anju Filter Coffee, naalu Cheesecake, moonu Cheese Corn Nuggets\"\n",
    "\n",
    "# Load audio at 16kHz\n",
    "waveform, sr = librosa.load(AUDIO_PATH, sr=16000)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Transcription function\n",
    "# --------------------------------------------------\n",
    "def transcribe_single(model, processor, audio, reference=None, label=\"\"):\n",
    "    inputs = processor(audio, sampling_rate=16000, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        predicted_ids = model.generate(input_features=inputs.input_features)\n",
    "    pred_text = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0].strip()\n",
    "\n",
    "    print(f\"\\n🔹 {label} Prediction: {pred_text}\")\n",
    "    if reference:\n",
    "        print(f\"   📝 Reference: {reference}\")\n",
    "        wer = wer_metric.compute(predictions=[pred_text], references=[reference])\n",
    "        print(f\"   📊 WER: {wer:.3f}\")\n",
    "\n",
    "    return pred_text\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 1. Base model (no LoRA)\n",
    "# --------------------------------------------------\n",
    "print(\"\\n=== BASE MODEL ===\")\n",
    "base_model = WhisperForConditionalGeneration.from_pretrained(BASE_MODEL)\n",
    "base_model.generation_config.forced_decoder_ids = None\n",
    "base_model.generation_config.suppress_tokens = []\n",
    "base_model.generation_config.language = \"en\"\n",
    "base_model.generation_config.task = \"transcribe\"\n",
    "base_model.eval()\n",
    "\n",
    "transcribe_single(base_model, processor, waveform, REFERENCE, \"Base\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 2. Fine-tuned model (with LoRA adapter)\n",
    "# --------------------------------------------------\n",
    "print(\"\\n=== FINE-TUNED MODEL (LoRA) ===\")\n",
    "model.generation_config.forced_decoder_ids = None\n",
    "model.generation_config.suppress_tokens = []\n",
    "model.generation_config.language = \"en\"\n",
    "model.generation_config.task = \"transcribe\"\n",
    "\n",
    "transcribe_single(model, processor, waveform, REFERENCE, \"Fine-tuned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
